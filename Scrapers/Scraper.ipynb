{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccd5d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from pathlib import Path\n",
    "import re\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import aiohttp\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52e2d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d969b731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Players Name/IDs from https://www.mlb.com/players\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "url = \"https://www.mlb.com/players\"\n",
    "driver.get(url)\n",
    "time.sleep(10)\n",
    "html = driver.page_source\n",
    "output_path = \"data/mlb_players_raw.html\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html)\n",
    "driver.quit()\n",
    "print(\"Downloaded full HTML with JavaScript saved to\", output_path)\n",
    "\n",
    "# Parse All Player ID's\n",
    "html_path = \"data/mlb_players_raw.html\"\n",
    "output_csv = \"data/mlb_players.csv\"\n",
    "with open(html_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    soup = BeautifulSoup(f, \"html.parser\")\n",
    "pattern = re.compile(r\"^/player/([a-z0-9\\-]+-\\d+)$\", re.IGNORECASE)\n",
    "anchors = soup.find_all(\"a\", class_=\"p-related-links__link\")\n",
    "player_entries = []\n",
    "for a in anchors:\n",
    "    href = a.get(\"href\", \"\")\n",
    "    text = a.get_text(strip=True)\n",
    "    match = pattern.match(href)\n",
    "    if match:\n",
    "        raw_slug = match.group(1)\n",
    "        player_entries.append([raw_slug, text])\n",
    "unique = {slug: name for slug, name in player_entries}\n",
    "unique_entries = [[slug, name] for slug, name in unique.items()]\n",
    "with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Player ID\", \"Player Name\"])\n",
    "    writer.writerows(unique_entries)\n",
    "df = pd.read_csv(output_csv)\n",
    "print(\"Sample of extracted player data:\")\n",
    "print(df.head(20))\n",
    "print(f\"Total unique players extracted: {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eca5bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Web Version Directly ^\n",
    "\n",
    "# url = \"https://www.mlb.com/players\"\n",
    "# output_csv = \"data/mlb_players.csv\"\n",
    "\n",
    "# chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")\n",
    "# chrome_options.add_argument(\"--disable-gpu\")\n",
    "# # chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "# driver = webdriver.Chrome(options=chrome_options)\n",
    "# print(\"Loading the page...\")\n",
    "# driver.get(url)\n",
    "# time.sleep(10)\n",
    "# driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "# time.sleep(5)\n",
    "# print(\"Extracting player link elements...\")\n",
    "\n",
    "# player_elements = driver.find_elements(By.CSS_SELECTOR, \"a.p-related-links__link\")\n",
    "# pattern = re.compile(r\"/player/([a-z0-9\\-]+-\\d+)\", re.IGNORECASE)\n",
    "# player_data = []\n",
    "# for elem in player_elements:\n",
    "#     href = elem.get_attribute(\"href\")\n",
    "#     text = elem.text.strip()\n",
    "#     match = pattern.search(href)\n",
    "#     if match:\n",
    "#         raw_slug = match.group(1)\n",
    "#         player_data.append([raw_slug, text])\n",
    "# print(f\"Found {len(player_data)} player entries.\")\n",
    "# driver.quit()  # Close the browser\n",
    "\n",
    "# # --- Write the extracted data to CSV ---\n",
    "# print(f\"Writing data to {output_csv}...\")\n",
    "# with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow([\"Player ID\", \"Player Name\"])\n",
    "#     writer.writerows(player_data)\n",
    "\n",
    "# # --- Preview the results using pandas ---\n",
    "# df = pd.read_csv(output_csv)\n",
    "# print(\"Sample of extracted player data:\")\n",
    "# print(df.head(20))\n",
    "# print(f\"Total unique players extracted: {df.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71319ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Download Player Batting Pages\n",
    "# From baseballsavant.mlb.com/savant-player<player-id>?stats=statcast-r-hitting-mlb\n",
    "\n",
    "# Download player batting pages\n",
    "nest_asyncio.apply()\n",
    "csv_path = \"data/mlb_players.csv\"\n",
    "output_folder = Path(\"data/raw/players-batting\")\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "df = pd.read_csv(csv_path)\n",
    "async def fetch(session, url, raw_slug, player_name, max_retries=3):\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            print(f\"Fetching: {player_name} -> {url} (Attempt {attempt})\")\n",
    "            async with session.get(url, timeout=15) as response:\n",
    "                if response.status == 200:\n",
    "                    text = await response.text()\n",
    "                    output_file = output_folder / f\"{raw_slug}.html\"\n",
    "                    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                        f.write(text)\n",
    "                    print(f\"Successfully fetched {player_name}\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"⚠️ Failed: {player_name} (HTTP {response.status}).\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error fetching {url} on attempt {attempt}: {e}\")\n",
    "        if attempt < max_retries:\n",
    "            await asyncio.sleep(1)  # Wait a bit before retrying\n",
    "        else:\n",
    "            print(f\"❌ Giving up on {player_name} after {max_retries} attempts.\")\n",
    "\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for _, row in df.iterrows():\n",
    "            raw_slug = row[\"Player ID\"]     # e.g., \"mike-yastrzemski-573262\"\n",
    "            player_name = row[\"Player Name\"]\n",
    "            url = f\"https://baseballsavant.mlb.com/savant-player/{raw_slug}?stats=statcast-r-hitting-mlb\"\n",
    "            tasks.append(fetch(session, url, raw_slug, player_name))\n",
    "        await asyncio.gather(*tasks)\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "    \n",
    "# v2 #\n",
    "# csv_path = \"data/mlb_players.csv\"\n",
    "# output_folder = Path(\"data/raw/players-batting\")\n",
    "# output_folder.mkdir(parents=True, exist_ok=True)\n",
    "# df = pd.read_csv(csv_path)\n",
    "# options = Options()\n",
    "# options.add_argument(\"--headless\")\n",
    "# options.add_argument(\"--disable-gpu\")\n",
    "# driver = webdriver.Chrome(options=options)\n",
    "# skipped = 0\n",
    "# downloaded = 0\n",
    "# for _, row in df.iterrows():\n",
    "#     raw_slug = row[\"Player ID\"]\n",
    "#     player_name = row[\"Player Name\"]\n",
    "#     output_file = output_folder / f\"{raw_slug}.html\"\n",
    "#     if output_file.exists():\n",
    "#         print(f\"Skipping: {player_name} (already downloaded)\")\n",
    "#         skipped += 1\n",
    "#         continue\n",
    "        \n",
    "#     url = f\"https://baseballsavant.mlb.com/savant-player/{raw_slug}?stats=statcast-r-hitting-mlb\"\n",
    "#     try:\n",
    "#         print(f\"Fetching: {player_name} -> {url}\")\n",
    "#         driver.get(url)\n",
    "#         html = driver.page_source\n",
    "#         with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#             f.write(html)\n",
    "#         downloaded += 1\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error fetching {url}: {e}\")\n",
    "# driver.quit()\n",
    "# print(f\"Completed downloading {len(df)} player pages to {output_folder}\")\n",
    "\n",
    "# v0 #\n",
    "# csv_path = \"data/mlb_players.csv\"\n",
    "# output_folder = Path(\"data/raw/players-batting\")\n",
    "# output_folder.mkdir(parents=True, exist_ok=True)\n",
    "# df = pd.read_csv(csv_path)\n",
    "# for idx, row in df.iterrows():\n",
    "#     raw_slug = row[\"Player ID\"]  # e.g., \"mike-yastrzemski-573262\"\n",
    "#     player_name = row[\"Player Name\"]\n",
    "\n",
    "#     url = f\"https://baseballsavant.mlb.com/savant-player/{raw_slug}?stats=statcast-r-hitting-mlb\"\n",
    "#     print(f\"[{idx+1}/{len(df)}] Fetching: {player_name} -> {url}\")\n",
    "\n",
    "#     try:\n",
    "#         response = requests.get(url, timeout=15)\n",
    "#         if response.status_code == 200:\n",
    "#             output_file = output_folder / f\"{raw_slug}.html\"\n",
    "#             with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#                 f.write(response.text)\n",
    "#         else:\n",
    "#             print(f\"⚠️ Failed: {player_name} (HTTP {response.status_code})\")\n",
    "#     except requests.RequestException as e:\n",
    "#         print(f\"❌ Error fetching {url}: {e}\")\n",
    "#     time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbe23629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1454 player files...\n",
      "Progress: 20/1454 files (1.4%)\n",
      "Progress: 40/1454 files (2.8%)\n",
      "Progress: 60/1454 files (4.1%)\n",
      "Progress: 80/1454 files (5.5%)\n",
      "Progress: 100/1454 files (6.9%)\n",
      "Progress: 120/1454 files (8.3%)\n",
      "Progress: 140/1454 files (9.6%)\n",
      "Progress: 160/1454 files (11.0%)\n",
      "Progress: 180/1454 files (12.4%)\n",
      "Progress: 200/1454 files (13.8%)\n",
      "Progress: 220/1454 files (15.1%)\n",
      "Progress: 240/1454 files (16.5%)\n",
      "Progress: 260/1454 files (17.9%)\n",
      "Progress: 280/1454 files (19.3%)\n",
      "Progress: 300/1454 files (20.6%)\n",
      "Progress: 320/1454 files (22.0%)\n",
      "Progress: 340/1454 files (23.4%)\n",
      "Progress: 360/1454 files (24.8%)\n",
      "Progress: 380/1454 files (26.1%)\n",
      "Progress: 400/1454 files (27.5%)\n",
      "Progress: 420/1454 files (28.9%)\n",
      "Progress: 440/1454 files (30.3%)\n",
      "Progress: 460/1454 files (31.6%)\n",
      "Progress: 480/1454 files (33.0%)\n",
      "Progress: 500/1454 files (34.4%)\n",
      "Progress: 520/1454 files (35.8%)\n",
      "Progress: 540/1454 files (37.1%)\n",
      "Progress: 560/1454 files (38.5%)\n",
      "Progress: 580/1454 files (39.9%)\n",
      "Progress: 600/1454 files (41.3%)\n",
      "Progress: 620/1454 files (42.6%)\n",
      "Progress: 640/1454 files (44.0%)\n",
      "Progress: 660/1454 files (45.4%)\n",
      "Progress: 680/1454 files (46.8%)\n",
      "Progress: 700/1454 files (48.1%)\n",
      "Progress: 720/1454 files (49.5%)\n",
      "Progress: 740/1454 files (50.9%)\n",
      "Progress: 760/1454 files (52.3%)\n",
      "Progress: 780/1454 files (53.6%)\n",
      "Progress: 800/1454 files (55.0%)\n",
      "Progress: 820/1454 files (56.4%)\n",
      "Progress: 840/1454 files (57.8%)\n",
      "Progress: 860/1454 files (59.1%)\n",
      "Progress: 880/1454 files (60.5%)\n",
      "Progress: 900/1454 files (61.9%)\n",
      "Progress: 920/1454 files (63.3%)\n",
      "Progress: 940/1454 files (64.6%)\n",
      "Progress: 960/1454 files (66.0%)\n",
      "Progress: 980/1454 files (67.4%)\n",
      "Progress: 1000/1454 files (68.8%)\n",
      "Progress: 1020/1454 files (70.2%)\n",
      "Progress: 1040/1454 files (71.5%)\n",
      "Progress: 1060/1454 files (72.9%)\n",
      "Progress: 1080/1454 files (74.3%)\n",
      "Progress: 1100/1454 files (75.7%)\n",
      "Progress: 1120/1454 files (77.0%)\n",
      "Progress: 1140/1454 files (78.4%)\n",
      "Progress: 1160/1454 files (79.8%)\n",
      "Progress: 1180/1454 files (81.2%)\n",
      "Progress: 1200/1454 files (82.5%)\n",
      "Progress: 1220/1454 files (83.9%)\n",
      "Progress: 1240/1454 files (85.3%)\n",
      "Progress: 1260/1454 files (86.7%)\n",
      "Progress: 1280/1454 files (88.0%)\n",
      "Progress: 1300/1454 files (89.4%)\n",
      "Progress: 1320/1454 files (90.8%)\n",
      "Progress: 1340/1454 files (92.2%)\n",
      "Progress: 1360/1454 files (93.5%)\n",
      "Progress: 1380/1454 files (94.9%)\n",
      "Progress: 1400/1454 files (96.3%)\n",
      "Progress: 1420/1454 files (97.7%)\n",
      "Progress: 1440/1454 files (99.0%)\n",
      "Progress: 1454/1454 files (100.0%)\n",
      "✅ Saved 1454 players to data/player_general_info.csv\n"
     ]
    }
   ],
   "source": [
    "### Parse For General Player Info ###\n",
    "\n",
    "input_folder = Path(\"data/raw/players-batting\")\n",
    "output_csv = \"data/player_general_info.csv\"\n",
    "rows = []\n",
    "all_files = list(input_folder.glob(\"*.html\"))\n",
    "total_files = len(all_files)\n",
    "processed = 0\n",
    "print(f\"Processing {total_files} player files...\")\n",
    "\n",
    "# Change the loop to use all_files\n",
    "# for file in input_folder.glob(\"*.html\"):\n",
    "for file in all_files:\n",
    "    processed += 1\n",
    "    if processed % 20 == 0 or processed == total_files:\n",
    "        print(f\"Progress: {processed}/{total_files} files ({processed/total_files*100:.1f}%)\")\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "    bio_section = soup.find(\"div\", class_=\"bio-player-name\")\n",
    "    if not bio_section:\n",
    "        continue\n",
    "    player_name = bio_section.find(\"div\").get_text(strip=True)\n",
    "    subsections = bio_section.find_all(\"div\", style=lambda v: v and \"font-size: .8rem\" in v)\n",
    "    if len(subsections) < 1:\n",
    "        continue\n",
    "    line1 = subsections[0].get_text(\" \", strip=True)\n",
    "    parts = [p.strip() for p in line1.split(\"|\")]\n",
    "    if len(parts) < 4:\n",
    "        continue\n",
    "    position = parts[0]\n",
    "    bats_throws = parts[1].replace(\"Bats/Throws:\", \"\").strip()\n",
    "    height_weight = parts[2].strip()\n",
    "    age = parts[3].replace(\"Age:\", \"\").strip()\n",
    "    hw_parts = height_weight.split()\n",
    "    height = hw_parts[0] + \" \" + hw_parts[1]\n",
    "    weight = hw_parts[2] if len(hw_parts) > 2 else \"\"\n",
    "    player_id = file.stem\n",
    "    rows.append({\n",
    "        \"Player ID\": player_id,\n",
    "        \"Position\": position,\n",
    "        \"Bats/Throws\": bats_throws,\n",
    "        \"Height\": height,\n",
    "        \"Weight\": weight,\n",
    "        \"Age\": age\n",
    "    })\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ Saved {len(df)} players to {output_csv}\")\n",
    "\n",
    "!open data/player_general_info.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2578114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1454 player files for batting stats...\n",
      "Progress: 20/1454 files (1.4%)\n",
      "Progress: 40/1454 files (2.8%)\n",
      "Progress: 60/1454 files (4.1%)\n",
      "Progress: 80/1454 files (5.5%)\n",
      "Progress: 100/1454 files (6.9%)\n",
      "Progress: 120/1454 files (8.3%)\n",
      "Progress: 140/1454 files (9.6%)\n",
      "Progress: 160/1454 files (11.0%)\n",
      "Progress: 180/1454 files (12.4%)\n",
      "Progress: 200/1454 files (13.8%)\n",
      "Progress: 220/1454 files (15.1%)\n",
      "Progress: 240/1454 files (16.5%)\n",
      "Progress: 260/1454 files (17.9%)\n",
      "Progress: 280/1454 files (19.3%)\n",
      "Progress: 300/1454 files (20.6%)\n",
      "Progress: 320/1454 files (22.0%)\n",
      "Progress: 340/1454 files (23.4%)\n",
      "Progress: 360/1454 files (24.8%)\n",
      "Progress: 380/1454 files (26.1%)\n",
      "Progress: 400/1454 files (27.5%)\n",
      "Progress: 420/1454 files (28.9%)\n",
      "Progress: 440/1454 files (30.3%)\n",
      "Progress: 460/1454 files (31.6%)\n",
      "Progress: 480/1454 files (33.0%)\n",
      "Progress: 500/1454 files (34.4%)\n",
      "Progress: 520/1454 files (35.8%)\n",
      "Progress: 540/1454 files (37.1%)\n",
      "Progress: 560/1454 files (38.5%)\n",
      "Progress: 580/1454 files (39.9%)\n",
      "Progress: 600/1454 files (41.3%)\n",
      "Progress: 620/1454 files (42.6%)\n",
      "Progress: 640/1454 files (44.0%)\n",
      "Progress: 660/1454 files (45.4%)\n",
      "Progress: 680/1454 files (46.8%)\n",
      "Progress: 700/1454 files (48.1%)\n",
      "Progress: 720/1454 files (49.5%)\n",
      "Progress: 740/1454 files (50.9%)\n",
      "Progress: 760/1454 files (52.3%)\n",
      "Progress: 780/1454 files (53.6%)\n",
      "Progress: 800/1454 files (55.0%)\n",
      "Progress: 820/1454 files (56.4%)\n",
      "Progress: 840/1454 files (57.8%)\n",
      "Progress: 860/1454 files (59.1%)\n",
      "Progress: 880/1454 files (60.5%)\n",
      "Progress: 900/1454 files (61.9%)\n",
      "Progress: 920/1454 files (63.3%)\n",
      "Progress: 940/1454 files (64.6%)\n",
      "Progress: 960/1454 files (66.0%)\n",
      "Progress: 980/1454 files (67.4%)\n",
      "Progress: 1000/1454 files (68.8%)\n",
      "Progress: 1020/1454 files (70.2%)\n",
      "Progress: 1040/1454 files (71.5%)\n",
      "Progress: 1060/1454 files (72.9%)\n",
      "Progress: 1080/1454 files (74.3%)\n",
      "Progress: 1100/1454 files (75.7%)\n",
      "Progress: 1120/1454 files (77.0%)\n",
      "Progress: 1140/1454 files (78.4%)\n",
      "Progress: 1160/1454 files (79.8%)\n",
      "Progress: 1180/1454 files (81.2%)\n",
      "Progress: 1200/1454 files (82.5%)\n",
      "Progress: 1220/1454 files (83.9%)\n",
      "Progress: 1240/1454 files (85.3%)\n",
      "Progress: 1260/1454 files (86.7%)\n",
      "Progress: 1280/1454 files (88.0%)\n",
      "Progress: 1300/1454 files (89.4%)\n",
      "Progress: 1320/1454 files (90.8%)\n",
      "Progress: 1340/1454 files (92.2%)\n",
      "Progress: 1360/1454 files (93.5%)\n",
      "Progress: 1380/1454 files (94.9%)\n",
      "Progress: 1400/1454 files (96.3%)\n",
      "Progress: 1420/1454 files (97.7%)\n",
      "Progress: 1440/1454 files (99.0%)\n",
      "Progress: 1454/1454 files (100.0%)\n",
      "✅ Saved 4746 rows to data/player_batting_stats.csv\n"
     ]
    }
   ],
   "source": [
    "### Parse for Batting Statistics Table ###\n",
    "\n",
    "input_folder = Path(\"data/raw/players-batting\")\n",
    "output_csv = \"data/player_batting_stats.csv\"\n",
    "all_rows = []\n",
    "\n",
    "all_files = list(input_folder.glob(\"*.html\"))\n",
    "total_files = len(all_files)\n",
    "processed = 0\n",
    "success = 0\n",
    "print(f\"Processing {total_files} player files for batting stats...\")\n",
    "for file_path in all_files:\n",
    "    processed += 1\n",
    "    if processed % 20 == 0 or processed == total_files:\n",
    "        print(f\"Progress: {processed}/{total_files} files ({processed/total_files*100:.1f}%)\")\n",
    "        \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "    slug = file_path.stem\n",
    "    table_div = soup.find(\"div\", id=\"statcast_glance_batter\")\n",
    "    if not table_div:\n",
    "        continue\n",
    "    table = table_div.find(\"table\")\n",
    "    if not table:\n",
    "        continue\n",
    "    headers = [th.get_text(strip=True).replace(\"\\n\", \" \") for th in table.find(\"thead\").find_all(\"th\")]\n",
    "    for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "        cells = tr.find_all(\"td\")\n",
    "        row_data = [td.get_text(strip=True) for td in cells]\n",
    "        if len(row_data) == len(headers):\n",
    "            row_dict = dict(zip(headers, row_data))\n",
    "            row_dict[\"Player ID\"] = slug\n",
    "            all_rows.append(row_dict)\n",
    "            success += 1\n",
    "\n",
    "df = pd.DataFrame(all_rows)\n",
    "df = df[~df['Season'].isin(['Player', 'MLB'])]\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ Saved {len(df)} rows to {output_csv}\")\n",
    "\n",
    "!open data/player_batting_stats.csv\n",
    "\n",
    "# for file_path in input_folder.glob(\"*.html\"):\n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         soup = BeautifulSoup(f, \"html.parser\")\n",
    "#     slug = file_path.stem\n",
    "#     table_div = soup.find(\"div\", id=\"statcast_glance_batter\")\n",
    "#     if not table_div:\n",
    "#         continue\n",
    "#     table = table_div.find(\"table\")\n",
    "#     if not table:\n",
    "#         continue\n",
    "#     headers = [th.get_text(strip=True).replace(\"\\n\", \" \") for th in table.find(\"thead\").find_all(\"th\")]\n",
    "#     for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "#         cells = tr.find_all(\"td\")\n",
    "#         row_data = [td.get_text(strip=True) for td in cells]\n",
    "#         if len(row_data) == len(headers):\n",
    "#             row_dict = dict(zip(headers, row_data))\n",
    "#             row_dict[\"Player Slug\"] = slug\n",
    "#             all_rows.append(row_dict)\n",
    "# df = pd.DataFrame(all_rows)\n",
    "# df.to_csv(output_csv, index=False)\n",
    "# print(f\"✅ Saved {len(df)} rows to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8414d191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Processing 1454 players using 20 workers (41 chunks)...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Players:   0%|          | 1/1454 [00:03<1:32:30,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Andrew Abbott: Message: \n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000100982dec cxxbridge1$str$ptr + 2817040\n",
      "1   chromedriver                        0x000000010097b088 cxxbridge1$str$ptr + 2784940\n",
      "2   chromedriver                        0x00000001004c28d8 cxxbridge1$string$len + 93028\n",
      "3   chromedriver                        0x00000001005096a0 cxxbridge1$string$len + 383276\n",
      "4   chromedriver                        0x000000010054a7b8 cxxbridge1$string$len + 649796\n",
      "5   chromedriver                        0x00000001004fda80 cxxbridge1$string$len + 335116\n",
      "6   chromedriver                        0x0000000100947c74 cxxbridge1$str$ptr + 2575000\n",
      "7   chromedriver                        0x000000010094af40 cxxbridge1$str$ptr + 2588004\n",
      "8   chromedriver                        0x00000001009279fc cxxbridge1$str$ptr + 2443296\n",
      "9   chromedriver                        0x000000010094b7bc cxxbridge1$str$ptr + 2590176\n",
      "10  chromedriver                        0x0000000100918af0 cxxbridge1$str$ptr + 2382100\n",
      "11  chromedriver                        0x000000010096b9a0 cxxbridge1$str$ptr + 2721732\n",
      "12  chromedriver                        0x000000010096bb2c cxxbridge1$str$ptr + 2722128\n",
      "13  chromedriver                        0x000000010097acd4 cxxbridge1$str$ptr + 2783992\n",
      "14  libsystem_pthread.dylib             0x000000019f866c0c _pthread_start + 136\n",
      "15  libsystem_pthread.dylib             0x000000019f861b80 thread_start + 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Players:   0%|          | 3/1454 [00:07<54:47,  2.27s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Bryan Abreu: Message: \n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000100982dec cxxbridge1$str$ptr + 2817040\n",
      "1   chromedriver                        0x000000010097b088 cxxbridge1$str$ptr + 2784940\n",
      "2   chromedriver                        0x00000001004c28d8 cxxbridge1$string$len + 93028\n",
      "3   chromedriver                        0x00000001005096a0 cxxbridge1$string$len + 383276\n",
      "4   chromedriver                        0x000000010054a7b8 cxxbridge1$string$len + 649796\n",
      "5   chromedriver                        0x00000001004fda80 cxxbridge1$string$len + 335116\n",
      "6   chromedriver                        0x0000000100947c74 cxxbridge1$str$ptr + 2575000\n",
      "7   chromedriver                        0x000000010094af40 cxxbridge1$str$ptr + 2588004\n",
      "8   chromedriver                        0x00000001009279fc cxxbridge1$str$ptr + 2443296\n",
      "9   chromedriver                        0x000000010094b7bc cxxbridge1$str$ptr + 2590176\n",
      "10  chromedriver                        0x0000000100918af0 cxxbridge1$str$ptr + 2382100\n",
      "11  chromedriver                        0x000000010096b9a0 cxxbridge1$str$ptr + 2721732\n",
      "12  chromedriver                        0x000000010096bb2c cxxbridge1$str$ptr + 2722128\n",
      "13  chromedriver                        0x000000010097acd4 cxxbridge1$str$ptr + 2783992\n",
      "14  libsystem_pthread.dylib             0x000000019f866c0c _pthread_start + 136\n",
      "15  libsystem_pthread.dylib             0x000000019f861b80 thread_start + 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Players:   1%|          | 8/1454 [00:15<43:22,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Jason Adam: Message: \n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000100982dec cxxbridge1$str$ptr + 2817040\n",
      "1   chromedriver                        0x000000010097b088 cxxbridge1$str$ptr + 2784940\n",
      "2   chromedriver                        0x00000001004c28d8 cxxbridge1$string$len + 93028\n",
      "3   chromedriver                        0x00000001005096a0 cxxbridge1$string$len + 383276\n",
      "4   chromedriver                        0x000000010054a7b8 cxxbridge1$string$len + 649796\n",
      "5   chromedriver                        0x00000001004fda80 cxxbridge1$string$len + 335116\n",
      "6   chromedriver                        0x0000000100947c74 cxxbridge1$str$ptr + 2575000\n",
      "7   chromedriver                        0x000000010094af40 cxxbridge1$str$ptr + 2588004\n",
      "8   chromedriver                        0x00000001009279fc cxxbridge1$str$ptr + 2443296\n",
      "9   chromedriver                        0x000000010094b7bc cxxbridge1$str$ptr + 2590176\n",
      "10  chromedriver                        0x0000000100918af0 cxxbridge1$str$ptr + 2382100\n",
      "11  chromedriver                        0x000000010096b9a0 cxxbridge1$str$ptr + 2721732\n",
      "12  chromedriver                        0x000000010096bb2c cxxbridge1$str$ptr + 2722128\n",
      "13  chromedriver                        0x000000010097acd4 cxxbridge1$str$ptr + 2783992\n",
      "14  libsystem_pthread.dylib             0x000000019f866c0c _pthread_start + 136\n",
      "15  libsystem_pthread.dylib             0x000000019f861b80 thread_start + 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Players:   1%|          | 10/1454 [00:19<46:14,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Austin Adams: Message: \n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000100982dec cxxbridge1$str$ptr + 2817040\n",
      "1   chromedriver                        0x000000010097b088 cxxbridge1$str$ptr + 2784940\n",
      "2   chromedriver                        0x00000001004c28d8 cxxbridge1$string$len + 93028\n",
      "3   chromedriver                        0x00000001005096a0 cxxbridge1$string$len + 383276\n",
      "4   chromedriver                        0x000000010054a7b8 cxxbridge1$string$len + 649796\n",
      "5   chromedriver                        0x00000001004fda80 cxxbridge1$string$len + 335116\n",
      "6   chromedriver                        0x0000000100947c74 cxxbridge1$str$ptr + 2575000\n",
      "7   chromedriver                        0x000000010094af40 cxxbridge1$str$ptr + 2588004\n",
      "8   chromedriver                        0x00000001009279fc cxxbridge1$str$ptr + 2443296\n",
      "9   chromedriver                        0x000000010094b7bc cxxbridge1$str$ptr + 2590176\n",
      "10  chromedriver                        0x0000000100918af0 cxxbridge1$str$ptr + 2382100\n",
      "11  chromedriver                        0x000000010096b9a0 cxxbridge1$str$ptr + 2721732\n",
      "12  chromedriver                        0x000000010096bb2c cxxbridge1$str$ptr + 2722128\n",
      "13  chromedriver                        0x000000010097acd4 cxxbridge1$str$ptr + 2783992\n",
      "14  libsystem_pthread.dylib             0x000000019f866c0c _pthread_start + 136\n",
      "15  libsystem_pthread.dylib             0x000000019f861b80 thread_start + 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Players:   1%|          | 13/1454 [00:24<42:17,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Ty Adcock: Message: \n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000100982dec cxxbridge1$str$ptr + 2817040\n",
      "1   chromedriver                        0x000000010097b088 cxxbridge1$str$ptr + 2784940\n",
      "2   chromedriver                        0x00000001004c28d8 cxxbridge1$string$len + 93028\n",
      "3   chromedriver                        0x00000001005096a0 cxxbridge1$string$len + 383276\n",
      "4   chromedriver                        0x000000010054a7b8 cxxbridge1$string$len + 649796\n",
      "5   chromedriver                        0x00000001004fda80 cxxbridge1$string$len + 335116\n",
      "6   chromedriver                        0x0000000100947c74 cxxbridge1$str$ptr + 2575000\n",
      "7   chromedriver                        0x000000010094af40 cxxbridge1$str$ptr + 2588004\n",
      "8   chromedriver                        0x00000001009279fc cxxbridge1$str$ptr + 2443296\n",
      "9   chromedriver                        0x000000010094b7bc cxxbridge1$str$ptr + 2590176\n",
      "10  chromedriver                        0x0000000100918af0 cxxbridge1$str$ptr + 2382100\n",
      "11  chromedriver                        0x000000010096b9a0 cxxbridge1$str$ptr + 2721732\n",
      "12  chromedriver                        0x000000010096bb2c cxxbridge1$str$ptr + 2722128\n",
      "13  chromedriver                        0x000000010097acd4 cxxbridge1$str$ptr + 2783992\n",
      "14  libsystem_pthread.dylib             0x000000019f866c0c _pthread_start + 136\n",
      "15  libsystem_pthread.dylib             0x000000019f861b80 thread_start + 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Players:   1%|          | 15/1454 [00:28<44:37,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Joan Adon: Message: \n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000100982dec cxxbridge1$str$ptr + 2817040\n",
      "1   chromedriver                        0x000000010097b088 cxxbridge1$str$ptr + 2784940\n",
      "2   chromedriver                        0x00000001004c28d8 cxxbridge1$string$len + 93028\n",
      "3   chromedriver                        0x00000001005096a0 cxxbridge1$string$len + 383276\n",
      "4   chromedriver                        0x000000010054a7b8 cxxbridge1$string$len + 649796\n",
      "5   chromedriver                        0x00000001004fda80 cxxbridge1$string$len + 335116\n",
      "6   chromedriver                        0x0000000100947c74 cxxbridge1$str$ptr + 2575000\n",
      "7   chromedriver                        0x000000010094af40 cxxbridge1$str$ptr + 2588004\n",
      "8   chromedriver                        0x00000001009279fc cxxbridge1$str$ptr + 2443296\n",
      "9   chromedriver                        0x000000010094b7bc cxxbridge1$str$ptr + 2590176\n",
      "10  chromedriver                        0x0000000100918af0 cxxbridge1$str$ptr + 2382100\n",
      "11  chromedriver                        0x000000010096b9a0 cxxbridge1$str$ptr + 2721732\n",
      "12  chromedriver                        0x000000010096bb2c cxxbridge1$str$ptr + 2722128\n",
      "13  chromedriver                        0x000000010097acd4 cxxbridge1$str$ptr + 2783992\n",
      "14  libsystem_pthread.dylib             0x000000019f866c0c _pthread_start + 136\n",
      "15  libsystem_pthread.dylib             0x000000019f861b80 thread_start + 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Players:   1%|          | 17/1454 [00:32<49:24,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Julian Aguiar: Message: \n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000100982dec cxxbridge1$str$ptr + 2817040\n",
      "1   chromedriver                        0x000000010097b088 cxxbridge1$str$ptr + 2784940\n",
      "2   chromedriver                        0x00000001004c28d8 cxxbridge1$string$len + 93028\n",
      "3   chromedriver                        0x00000001005096a0 cxxbridge1$string$len + 383276\n",
      "4   chromedriver                        0x000000010054a7b8 cxxbridge1$string$len + 649796\n",
      "5   chromedriver                        0x00000001004fda80 cxxbridge1$string$len + 335116\n",
      "6   chromedriver                        0x0000000100947c74 cxxbridge1$str$ptr + 2575000\n",
      "7   chromedriver                        0x000000010094af40 cxxbridge1$str$ptr + 2588004\n",
      "8   chromedriver                        0x00000001009279fc cxxbridge1$str$ptr + 2443296\n",
      "9   chromedriver                        0x000000010094b7bc cxxbridge1$str$ptr + 2590176\n",
      "10  chromedriver                        0x0000000100918af0 cxxbridge1$str$ptr + 2382100\n",
      "11  chromedriver                        0x000000010096b9a0 cxxbridge1$str$ptr + 2721732\n",
      "12  chromedriver                        0x000000010096bb2c cxxbridge1$str$ptr + 2722128\n",
      "13  chromedriver                        0x000000010097acd4 cxxbridge1$str$ptr + 2783992\n",
      "14  libsystem_pthread.dylib             0x000000019f866c0c _pthread_start + 136\n",
      "15  libsystem_pthread.dylib             0x000000019f861b80 thread_start + 8\n",
      "\n",
      "\n",
      "\n",
      "⚠️ Process interrupted by user. Saving collected data...\n",
      "The file /Users/td/Code/mlb-ai/Scrapers/data/player_pitch_tracking_stats.csv does not exist.\n"
     ]
    }
   ],
   "source": [
    "### Parse for Pitch Tracking Table ###\n",
    "\n",
    "# start:\n",
    "# /html/body/div[2]/section/div/section/div[5]/section/div[1]/div[6]/div[2]/div/table/thead\n",
    "# /html/body/div[2]/section/div/section/div[5]/section/div[1]/div[6]/div[2]/div/table/thead/tr\n",
    "\n",
    "# column 1:\n",
    "# /html/body/div[2]/section/div/section/div[5]/section/div[1]/div[6]/div[2]/div/table/thead/tr/th[1]\n",
    "\n",
    "# columns 2:\n",
    "# /html/body/div[2]/section/div/section/div[5]/section/div[1]/div[6]/div[2]/div/table/thead/tr/th[2]\n",
    "\n",
    "# body:\n",
    "# /html/body/div[2]/section/div/section/div[5]/section/div[1]/div[6]/div[2]/div/table/tbody\n",
    "\n",
    "# row 1 values inside:\n",
    "# /html/body/div[2]/section/div/section/div[5]/section/div[1]/div[6]/div[2]/div/table/tbody/tr[1]\n",
    "\n",
    "# column 1 value\n",
    "# /html/body/div[2]/section/div/section/div[5]/section/div[1]/div[6]/div[2]/div/table/thead/tr/th[1]\n",
    "# /html/body/div[2]/section/div/section/div[5]/section/div[1]/div[6]/div[2]/div/table/thead/tr/th[1]/div\n",
    "\n",
    "# last column value\n",
    "# /html/body/div[2]/section/div/section/div[5]/section/div[1]/div[6]/div[2]/div/table/thead/tr/th[23]\n",
    "# /html/body/div[2]/section/div/section/div[5]/section/div[1]/div[6]/div[2]/div/table/thead/tr/th[23]/div\n",
    "\n",
    "\n",
    "# that is not the correct table for the last fucking time. This Pitch Tracking one is with these columns in the image ONLY \n",
    "# Year, Pitch Type, #, %, PA, AB, H, 1B, 2B, 3B, HR, SO, BBE, BA, XBA, SLG, XSLG, WOBA, XWOBA, EV, LA, Whiff%, PutAway%\n",
    "\n",
    "# Pitch Tracking\n",
    "# 2025\tFastball\t144\t49.0\t39\t36\t11\t8\t0\t0\t3\t5\t31\t.306\t.344\t.556\t.752\t.399\t.477\t95.9\t12\t15.9\t17.9\n",
    "# 2025\tBreaking\t104\t35.4\t25\t21\t5\t3\t1\t1\t0\t9\t12\t.238\t.290\t.381\t.449\t.335\t.378\t97.3\t10\t39.2\t22.0\n",
    "# 2025\tOffspeed\t46\t15.6\t11\t9\t2\t1\t0\t0\t1\t4\t5\t.222\t.280\t.556\t.550\t.399\t.413\t97.3\t7\t42.9\t33.3\n",
    "# 2024\tFastball\t1,520\t53.6\t395\t338\t105\t59\t23\t2\t21\t73\t268\t.311\t.326\t.577\t.636\t.414\t.444\t95.7\t15\t24.9\t20.4\n",
    "# 2024\tBreaking\t866\t30.5\t216\t195\t55\t21\t9\t2\t23\t67\t129\t.282\t.275\t.703\t.665\t.435\t.425\t95.0\t20\t38.1\t23.0\n",
    "# 2024\tOffspeed\t452\t15.9\t110\t103\t37\t18\t6\t3\t10\t22\t82\t.359\t.348\t.767\t.731\t.480\t.468\t96.8\t14\t29.5\t17.6\n",
    "\n",
    "\n",
    "\n",
    "# input_folder = Path(\"data/raw/players-batting\")\n",
    "# output_csv = \"data/player_pitch_tracking_stats.csv\"\n",
    "# all_pitch_rows = []\n",
    "# for file_path in input_folder.glob(\"*.html\"):\n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         soup = BeautifulSoup(f, \"html.parser\")\n",
    "#     slug = file_path.stem\n",
    "#     table = soup.find(\"table\", id=\"detailedPitches\")\n",
    "#     if not table:\n",
    "#         continue\n",
    "#     headers = [th.get_text(strip=True).replace(\"\\n\", \" \") for th in table.find(\"thead\").find_all(\"th\")]\n",
    "#     for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "#         tds = tr.find_all(\"td\")\n",
    "#         row = [td.get_text(strip=True) for td in tds]\n",
    "#         if len(row) == len(headers):\n",
    "#             row_dict = dict(zip(headers, row))\n",
    "#             row_dict[\"Player Slug\"] = slug\n",
    "#             all_pitch_rows.append(row_dict)\n",
    "# df_pitch = pd.DataFrame(all_pitch_rows)\n",
    "# df_pitch.to_csv(output_csv, index=False)\n",
    "# print(f\"✅ Saved {len(df_pitch)} rows to {output_csv}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# input_folder = Path(\"data/raw/players-batting\")\n",
    "# output_csv = \"data/player_pitch_tracking_stats.csv\"\n",
    "# all_rows = []\n",
    "\n",
    "# # Count total files first\n",
    "# all_files = list(input_folder.glob(\"*.html\"))\n",
    "# total_files = len(all_files)\n",
    "# processed = 0\n",
    "# success = 0\n",
    "\n",
    "# print(f\"Processing {total_files} player files for pitch tracking stats...\")\n",
    "\n",
    "# for file_path in all_files:\n",
    "#     processed += 1\n",
    "#     if processed % 20 == 0 or processed == total_files:\n",
    "#         print(f\"Progress: {processed}/{total_files} files ({processed/total_files*100:.1f}%)\")\n",
    "    \n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         soup = BeautifulSoup(f, \"html.parser\")\n",
    "    \n",
    "#     slug = file_path.stem\n",
    "#     table = soup.find(\"table\", {\"id\": \"detailedPitches\"})\n",
    "#     if not table:\n",
    "#         continue\n",
    "\n",
    "#     # Extract column headers\n",
    "#     headers = [th.get_text(strip=True).replace(\"\\n\", \" \") for th in table.find(\"thead\").find_all(\"th\")]\n",
    "\n",
    "#     # Extract data rows\n",
    "#     for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "#         cells = tr.find_all(\"td\")\n",
    "#         row_data = [td.get_text(strip=True).replace(\",\", \"\") for td in cells]\n",
    "#         if len(row_data) == len(headers):\n",
    "#             row_dict = dict(zip(headers, row_data))\n",
    "#             row_dict[\"Player Slug\"] = slug\n",
    "#             all_rows.append(row_dict)\n",
    "#             success += 1\n",
    "\n",
    "# df = pd.DataFrame(all_rows)\n",
    "# df.to_csv(output_csv, index=False)\n",
    "# print(f\"✅ Saved {len(df)} rows to {output_csv}\")\n",
    "\n",
    "### Parse for Pitch Tracking Table ###\n",
    "\n",
    "\n",
    "\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# from bs4 import BeautifulSoup\n",
    "# from pathlib import Path\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# input_folder = Path(\"data/raw/players-batting\")\n",
    "# output_csv = Path(\"data/player_pitch_tracking_stats.csv\")\n",
    "# all_rows = []\n",
    "\n",
    "# all_files = list(input_folder.glob(\"*.html\"))\n",
    "# total_files = len(all_files)\n",
    "\n",
    "# print(f\"\\n🔍 Scanning {input_folder} for player HTML files...\\n\")\n",
    "\n",
    "# for idx, file_path in enumerate(tqdm(all_files, desc=\"Processing Players\")):\n",
    "#     slug = file_path.stem\n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         soup = BeautifulSoup(f.read(), \"html.parser\")\n",
    "\n",
    "#     # Try to find the <h2> heading for Pitch Tracking\n",
    "#     pitch_heading = soup.find(\"h2\", string=lambda s: s and \"Pitch Tracking\" in s)\n",
    "#     if not pitch_heading:\n",
    "#         print(f\"❌ {slug}: 'Pitch Tracking' section not found\")\n",
    "#         continue\n",
    "\n",
    "#     # Look for the table after the heading\n",
    "#     table = None\n",
    "#     next_tag = pitch_heading.find_next_sibling()\n",
    "#     while next_tag:\n",
    "#         if next_tag.name == \"div\" and next_tag.find(\"table\"):\n",
    "#             table = next_tag.find(\"table\")\n",
    "#             break\n",
    "#         next_tag = next_tag.find_next_sibling()\n",
    "\n",
    "#     if not table:\n",
    "#         print(f\"❌ {slug}: Pitch Tracking table not found\")\n",
    "#         continue\n",
    "\n",
    "#     # Extract headers\n",
    "#     header_cells = table.find(\"thead\").find_all(\"th\")\n",
    "#     headers = [cell.get_text(strip=True) for cell in header_cells]\n",
    "\n",
    "#     # Extract data rows\n",
    "#     for row in table.find(\"tbody\").find_all(\"tr\"):\n",
    "#         cells = row.find_all(\"td\")\n",
    "#         if not cells:\n",
    "#             continue\n",
    "#         values = [td.get_text(strip=True) for td in cells]\n",
    "#         if len(values) != len(headers):\n",
    "#             continue\n",
    "#         row_dict = dict(zip(headers, values))\n",
    "#         row_dict[\"Player Slug\"] = slug\n",
    "#         all_rows.append(row_dict)\n",
    "\n",
    "# # Save output to CSV\n",
    "# df = pd.DataFrame(all_rows)\n",
    "# df.to_csv(output_csv, index=False)\n",
    "# print(f\"\\n✅ Done. Extracted {len(df)} total rows into {output_csv}\")\n",
    "\n",
    "\n",
    "\n",
    "# ASYNCIO VERSION # \n",
    "\n",
    "\n",
    "# import asyncio\n",
    "# import aiohttp\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "# from tqdm import tqdm\n",
    "# import time\n",
    "# import nest_asyncio\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "# from functools import partial\n",
    "\n",
    "# # Apply nest_asyncio to allow async in Jupyter\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# # Setup paths and read player data\n",
    "# df_players = pd.read_csv(\"data/mlb_players.csv\")\n",
    "# output_csv = \"data/player_detailed_pitches.csv\"\n",
    "# all_rows = []\n",
    "\n",
    "# # Setup Chrome options optimized for M1/M2/M4\n",
    "# options = Options()\n",
    "# options.add_argument(\"--headless=new\")  # New headless mode\n",
    "# options.add_argument(\"--disable-gpu\")\n",
    "# options.add_argument(\"--no-sandbox\")\n",
    "# options.add_argument(\"--disable-dev-shm-usage\")\n",
    "# options.add_argument(\"--disable-features=TranslateUI\")\n",
    "# options.add_argument(\"--disable-extensions\")\n",
    "# options.add_argument(\"--disable-component-extensions-with-background-pages\")\n",
    "# options.add_argument(\"--disable-default-apps\")\n",
    "# options.add_argument(\"--enable-features=NetworkService,NetworkServiceInProcess\")\n",
    "\n",
    "# # Configure chunk size for parallel processing\n",
    "# CHUNK_SIZE = 5  # Process 5 players simultaneously\n",
    "\n",
    "# async def process_player(player_id, player_name, driver):\n",
    "#     try:\n",
    "#         url = f\"https://baseballsavant.mlb.com/savant-player/{player_id}?stats=statcast-r-hitting-mlb\"\n",
    "        \n",
    "#         driver.get(url)\n",
    "#         await asyncio.sleep(1)  # Reduced wait time\n",
    "        \n",
    "#         soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        \n",
    "#         # Find table by its specific ID\n",
    "#         table = soup.find(\"table\", id=\"detailedPitches\")\n",
    "#         if not table:\n",
    "#             print(f\"No pitch data found for {player_id}\")\n",
    "#             return []\n",
    "            \n",
    "#         # Extract headers\n",
    "#         headers = []\n",
    "#         for th in table.find(\"thead\").find_all(\"th\"):\n",
    "#             header = th.get_text(strip=True)\n",
    "#             header = header.replace(\"\\n\", \" \").strip()\n",
    "#             headers.append(header)\n",
    "        \n",
    "#         player_rows = []\n",
    "#         # Process each row\n",
    "#         for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "#             cells = tr.find_all(\"td\")\n",
    "#             if not cells:\n",
    "#                 continue\n",
    "                \n",
    "#             row_data = {\"Player ID\": player_id}\n",
    "            \n",
    "#             for idx, cell in enumerate(cells):\n",
    "#                 if idx < len(headers):\n",
    "#                     value = cell.get_text(strip=True).replace(\",\", \"\")\n",
    "#                     row_data[headers[idx]] = value\n",
    "                    \n",
    "#             if len(row_data) > 1:\n",
    "#                 player_rows.append(row_data)\n",
    "                \n",
    "#         return player_rows\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error processing {player_id}: {str(e)}\")\n",
    "#         return []\n",
    "\n",
    "# async def process_chunk(chunk_df, chunk_idx):\n",
    "#     driver = webdriver.Chrome(options=options)\n",
    "#     chunk_rows = []\n",
    "    \n",
    "#     try:\n",
    "#         for _, row in chunk_df.iterrows():\n",
    "#             player_rows = await process_player(row[\"Player ID\"], row[\"Player Name\"], driver)\n",
    "#             chunk_rows.extend(player_rows)\n",
    "            \n",
    "#         return chunk_rows\n",
    "#     finally:\n",
    "#         driver.quit()\n",
    "\n",
    "# async def main():\n",
    "#     chunks = [df_players[i:i + CHUNK_SIZE] for i in range(0, len(df_players), CHUNK_SIZE)]\n",
    "#     print(f\"\\n🔍 Processing {len(df_players)} players in {len(chunks)} chunks...\\n\")\n",
    "    \n",
    "#     all_rows = []\n",
    "    \n",
    "#     for chunk_idx, chunk_df in enumerate(tqdm(chunks, desc=\"Processing Chunks\")):\n",
    "#         chunk_rows = await process_chunk(chunk_df, chunk_idx)\n",
    "#         all_rows.extend(chunk_rows)\n",
    "        \n",
    "#         # Save progress after each chunk\n",
    "#         if chunk_rows:\n",
    "#             temp_df = pd.DataFrame(all_rows)\n",
    "#             first_cols = [\"Player ID\", \"Year\", \"Pitch Type\"]\n",
    "#             other_cols = [col for col in temp_df.columns if col not in first_cols]\n",
    "#             temp_df = temp_df[first_cols + sorted(other_cols)]\n",
    "#             temp_df.to_csv(output_csv, index=False)\n",
    "#             print(f\"\\n💾 Progress saved: {len(all_rows)} total rows\")\n",
    "    \n",
    "#     return all_rows\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         all_rows = asyncio.run(main())\n",
    "        \n",
    "#         if all_rows:\n",
    "#             df = pd.DataFrame(all_rows)\n",
    "#             first_cols = [\"Player ID\", \"Year\", \"Pitch Type\"]\n",
    "#             other_cols = [col for col in df.columns if col not in first_cols]\n",
    "#             df = df[first_cols + sorted(other_cols)]\n",
    "#             df.to_csv(output_csv, index=False)\n",
    "#             print(f\"\\n✅ Saved {len(df)} rows to {output_csv}\")\n",
    "#         else:\n",
    "#             print(\"\\n⚠️ No data was collected\")\n",
    "            \n",
    "#     except KeyboardInterrupt:\n",
    "#         print(\"\\n\\n⚠️ Process interrupted by user. Saving collected data...\")\n",
    "#         if all_rows:\n",
    "#             df = pd.DataFrame(all_rows)\n",
    "#             df.to_csv(output_csv, index=False)\n",
    "#             print(f\"Saved {len(df)} rows to {output_csv}\")\n",
    "\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import nest_asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "# Apply nest_asyncio to allow async in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Setup paths and read player data\n",
    "df_players = pd.read_csv(\"data/mlb_players.csv\")\n",
    "output_csv = \"data/player_detailed_pitches.csv\"\n",
    "all_rows = []\n",
    "players_processed = 0\n",
    "\n",
    "# Define the exact columns we want in order\n",
    "COLUMNS = [\n",
    "    \"Player ID\", \"Year\", \"Pitch Type\", \"#\", \"%\", \"PA\", \"AB\", \"H\", \"1B\", \"2B\", \n",
    "    \"3B\", \"HR\", \"SO\", \"BBE\", \"BA\", \"XBA\", \"SLG\", \"XSLG\", \"WOBA\", \"XWOBA\", \n",
    "    \"EV\", \"LA\", \"Whiff%\", \"PutAway%\"\n",
    "]\n",
    "\n",
    "# Setup Chrome options optimized for M1/M2/M4\n",
    "options = Options()\n",
    "options.add_argument(\"--headless=new\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--disable-features=TranslateUI\")\n",
    "options.add_argument(\"--disable-extensions\")\n",
    "options.add_argument(\"--disable-component-extensions-with-background-pages\")\n",
    "options.add_argument(\"--disable-default-apps\")\n",
    "options.add_argument(\"--enable-features=NetworkService,NetworkServiceInProcess\")\n",
    "options.add_argument(\"--disable-javascript-harmony-shipping\")\n",
    "options.add_argument(\"--disable-site-isolation-trials\")\n",
    "options.add_argument(\"--ignore-certificate-errors\")\n",
    "options.add_argument(\"--disable-web-security\")\n",
    "options.add_argument(\"--disable-features=IsolateOrigins,site-per-process\")\n",
    "options.add_argument(\"--aggressive-cache-discard\")\n",
    "options.add_argument(\"--disable-cache\")\n",
    "options.add_argument(\"--disable-application-cache\")\n",
    "options.add_argument(\"--disable-offline-load-stale-cache\")\n",
    "options.add_argument(\"--disk-cache-size=0\")\n",
    "options.page_load_strategy = 'none'  # Changed from 'eager' to 'none' for fastest possible loading\n",
    "\n",
    "# Configure parallel processing\n",
    "NUM_WORKERS = 20  # Increased workers\n",
    "CHUNK_SIZE = max(1, len(df_players) // (NUM_WORKERS * 2))\n",
    "SAVE_FREQUENCY = 25  # Save every 25 players\n",
    "\n",
    "# Minimum viable timeouts (in seconds)\n",
    "PAGE_LOAD_TIMEOUT = 2\n",
    "SCRIPT_TIMEOUT = 2\n",
    "ELEMENT_TIMEOUT = 2\n",
    "\n",
    "# XPath constants\n",
    "TABLE_XPATH = \"/html/body/div[2]/section/div/section/div[5]/section/div[1]/div[6]/div[2]/div/table\"\n",
    "THEAD_XPATH = f\"{TABLE_XPATH}/thead/tr\"\n",
    "TBODY_XPATH = f\"{TABLE_XPATH}/tbody\"\n",
    "\n",
    "class PlayerDataExtractor:\n",
    "    def __init__(self):\n",
    "        self.driver = None\n",
    "        self.wait = None\n",
    "        \n",
    "    async def setup(self):\n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "        self.driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)\n",
    "        self.driver.set_script_timeout(SCRIPT_TIMEOUT)\n",
    "        self.wait = WebDriverWait(self.driver, ELEMENT_TIMEOUT)\n",
    "        \n",
    "    async def cleanup(self):\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            \n",
    "    async def get_table_data(self, player_id: str, player_name: str) -> List[Dict]:\n",
    "        try:\n",
    "            url = f\"https://baseballsavant.mlb.com/savant-player/{player_id}?stats=statcast-r-hitting-mlb\"\n",
    "            try:\n",
    "                self.driver.get(url)\n",
    "            except TimeoutException:\n",
    "                # If timeout occurs, the page might still be usable\n",
    "                pass\n",
    "                \n",
    "            try:\n",
    "                table = self.wait.until(EC.presence_of_element_located((By.XPATH, TABLE_XPATH)))\n",
    "            except TimeoutException:\n",
    "                # One more quick attempt with a shorter timeout\n",
    "                self.wait = WebDriverWait(self.driver, 1)\n",
    "                table = self.wait.until(EC.presence_of_element_located((By.XPATH, TABLE_XPATH)))\n",
    "            \n",
    "            # Extract headers immediately without waiting\n",
    "            header_map = {}\n",
    "            header_elements = self.driver.find_elements(By.XPATH, f\"{THEAD_XPATH}/th\")\n",
    "            for idx, elem in enumerate(header_elements):\n",
    "                try:\n",
    "                    header = elem.find_element(By.TAG_NAME, \"div\").text.strip()\n",
    "                except:\n",
    "                    header = elem.text.strip()\n",
    "                header = header.replace(\"\\n\", \" \").strip()\n",
    "                if header in COLUMNS:\n",
    "                    header_map[idx] = header\n",
    "            \n",
    "            # Extract rows without explicit wait\n",
    "            tbody = self.driver.find_element(By.XPATH, TBODY_XPATH)\n",
    "            rows = tbody.find_elements(By.TAG_NAME, \"tr\")\n",
    "            \n",
    "            player_rows = []\n",
    "            for row in rows:\n",
    "                cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                if not cells:\n",
    "                    continue\n",
    "                \n",
    "                row_data = {col: \"\" for col in COLUMNS}\n",
    "                row_data[\"Player ID\"] = player_id\n",
    "                has_data = False\n",
    "                \n",
    "                for idx, cell in enumerate(cells):\n",
    "                    if idx in header_map:\n",
    "                        value = cell.text.strip().replace(\",\", \"\")\n",
    "                        if value:\n",
    "                            row_data[header_map[idx]] = value\n",
    "                            has_data = True\n",
    "                \n",
    "                if has_data:\n",
    "                    player_rows.append(row_data)\n",
    "            \n",
    "            return player_rows\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {player_name}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "async def process_chunk(chunk_df, worker_id, progress_bar):\n",
    "    global players_processed, all_rows\n",
    "    extractor = PlayerDataExtractor()\n",
    "    chunk_rows = []\n",
    "    \n",
    "    try:\n",
    "        await extractor.setup()\n",
    "        \n",
    "        for _, row in chunk_df.iterrows():\n",
    "            try:\n",
    "                player_rows = await extractor.get_table_data(row[\"Player ID\"], row[\"Player Name\"])\n",
    "                if player_rows:\n",
    "                    chunk_rows.extend(player_rows)\n",
    "                players_processed += 1\n",
    "                progress_bar.update(1)\n",
    "                \n",
    "                # Save progress periodically\n",
    "                if players_processed % SAVE_FREQUENCY == 0:\n",
    "                    await save_progress(chunk_rows)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in worker {worker_id}: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "        return chunk_rows\n",
    "    finally:\n",
    "        await extractor.cleanup()\n",
    "\n",
    "async def save_progress(new_rows):\n",
    "    if not new_rows:\n",
    "        return\n",
    "        \n",
    "    temp_df = pd.DataFrame(all_rows + new_rows)\n",
    "    if not temp_df.empty:\n",
    "        temp_df = temp_df[COLUMNS]\n",
    "        temp_df.to_csv(output_csv, index=False)\n",
    "\n",
    "async def main():\n",
    "    chunks = [df_players[i:i + CHUNK_SIZE] for i in range(0, len(df_players), CHUNK_SIZE)]\n",
    "    print(f\"\\n🔍 Processing {len(df_players)} players using {NUM_WORKERS} workers ({len(chunks)} chunks)...\\n\")\n",
    "    \n",
    "    # Create progress bar for total players\n",
    "    progress_bar = tqdm(total=len(df_players), desc=\"Processing Players\")\n",
    "    \n",
    "    # Create semaphore to limit concurrent Chrome instances\n",
    "    sem = asyncio.Semaphore(NUM_WORKERS)\n",
    "    \n",
    "    async def process_chunk_with_semaphore(chunk_df, worker_id):\n",
    "        async with sem:\n",
    "            return await process_chunk(chunk_df, worker_id, progress_bar)\n",
    "    \n",
    "    # Process chunks concurrently\n",
    "    tasks = [\n",
    "        asyncio.create_task(process_chunk_with_semaphore(chunk_df, i))\n",
    "        for i, chunk_df in enumerate(chunks)\n",
    "    ]\n",
    "    \n",
    "    # Wait for all tasks to complete\n",
    "    chunk_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    # Combine results\n",
    "    for result in chunk_results:\n",
    "        if isinstance(result, list):\n",
    "            all_rows.extend(result)\n",
    "    \n",
    "    progress_bar.close()\n",
    "    return all_rows\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        all_rows = asyncio.run(main())\n",
    "        \n",
    "        if all_rows:\n",
    "            df = pd.DataFrame(all_rows)\n",
    "            df = df[COLUMNS]\n",
    "            df.to_csv(output_csv, index=False)\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"\\n✅ Completed in {elapsed_time:.2f} seconds\")\n",
    "            print(f\"✅ Processed {players_processed} players\")\n",
    "            print(f\"✅ Saved {len(df)} rows to {output_csv}\")\n",
    "        else:\n",
    "            print(\"\\n⚠️ No data was collected\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n⚠️ Process interrupted by user. Saving collected data...\")\n",
    "        if all_rows:\n",
    "            df = pd.DataFrame(all_rows)\n",
    "            df = df[COLUMNS]\n",
    "            df.to_csv(output_csv, index=False)\n",
    "            print(f\"Saved {len(df)} rows to {output_csv}\")\n",
    "            \n",
    "            \n",
    "!open data/player_pitch_tracking_stats.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca47267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Batted Ball Profile Table\n",
    "# # Quality Of Contact Table\n",
    "\n",
    "\n",
    "# # Input and output paths\n",
    "# input_folder = Path(\"data/raw/players-batting\")  # folder with .html player files\n",
    "# output_profile = \"data/player_batted_ball_profile.csv\"\n",
    "# output_quality = \"data/player_quality_of_contact.csv\"\n",
    "\n",
    "# all_profile_rows = []\n",
    "# all_quality_rows = []\n",
    "\n",
    "# # Helper function to parse tables\n",
    "# def parse_table(table, slug):\n",
    "#     headers = [th.get_text(strip=True) for th in table.find(\"thead\").find_all(\"th\")]\n",
    "#     rows = []\n",
    "#     for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "#         cells = tr.find_all([\"td\", \"th\"])\n",
    "#         values = [td.get_text(strip=True) for td in cells]\n",
    "#         if len(values) == len(headers):\n",
    "#             row_dict = dict(zip(headers, values))\n",
    "#             row_dict[\"Player Slug\"] = slug\n",
    "#             rows.append(row_dict)\n",
    "#     return rows\n",
    "\n",
    "# # Run test on uploaded files\n",
    "# html_files = [\n",
    "#     (\"aaron-judge-592450.html\", \"aaron-judge-592450\"),\n",
    "#     (\"brandon-lowe-664040.html\", \"brandon-lowe-664040\"),\n",
    "# ]\n",
    "\n",
    "# for filename, slug in html_files:\n",
    "#     with open(f\"/mnt/data/{filename}\", \"r\", encoding=\"utf-8\") as f:\n",
    "#         soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "#     tables = soup.find_all(\"table\")\n",
    "#     if len(tables) >= 2:\n",
    "#         all_profile_rows.extend(parse_table(tables[0], slug))\n",
    "#         all_quality_rows.extend(parse_table(tables[1], slug))\n",
    "\n",
    "# # Convert to DataFrames\n",
    "# df_profile = pd.DataFrame(all_profile_rows)\n",
    "# df_quality = pd.DataFrame(all_quality_rows)\n",
    "\n",
    "# # Show output here for verification\n",
    "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Batted Ball Profile Table\", dataframe=df_profile)\n",
    "# tools.display_dataframe_to_user(name=\"Quality of Contact Table\", dataframe=df_quality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c585e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Batted Ball v2\n",
    "\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# input_folder = Path(\"Scrapers/data/raw/players-batting\")\n",
    "# output_file = Path(\"data/player_pitch_tracking_stats.csv\")\n",
    "\n",
    "# all_rows = []\n",
    "# files_processed = 0\n",
    "# files_with_data = 0\n",
    "\n",
    "# # Keep track of the first found table's columns to use as reference\n",
    "# reference_columns = []\n",
    "\n",
    "# print(f\"🔍 Scanning {input_folder} for player HTML files...\\n\")\n",
    "\n",
    "# all_files = list(input_folder.glob(\"*.html\"))\n",
    "# print(f\"Found {len(all_files)} HTML files\")\n",
    "\n",
    "# for file_path in tqdm(all_files, desc=\"Processing players\"):\n",
    "#     files_processed += 1\n",
    "    \n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         soup = BeautifulSoup(f.read(), \"html.parser\")\n",
    "\n",
    "#     player_id = file_path.stem\n",
    "#     print(f\"\\nProcessing {player_id}...\")\n",
    "\n",
    "#     # First try the standard detailedPitches ID\n",
    "#     table = soup.find(\"table\", {\"id\": \"detailedPitches\"})\n",
    "\n",
    "#     # If not found, look for h2 elements containing \"Pitch Tracking\" text and find the next table\n",
    "#     if not table:\n",
    "#         pitch_tracking_headers = soup.find_all(lambda tag: tag.name == \"h2\" and \"Pitch Tracking\" in tag.get_text())\n",
    "#         for header in pitch_tracking_headers:\n",
    "#             table = header.find_next(\"table\")\n",
    "#             if table:\n",
    "#                 break\n",
    "\n",
    "#     # If still not found, search for tables with specific column headers\n",
    "#     if not table:\n",
    "#         candidate_tables = soup.find_all(\"table\")\n",
    "#         for t in candidate_tables:\n",
    "#             headers = [th.get_text(strip=True) for th in t.find_all(\"th\")]\n",
    "#             if \"Pitch Type\" in headers and \"Year\" in headers:\n",
    "#                 table = t\n",
    "#                 break\n",
    "\n",
    "#     if not table:\n",
    "#         print(f\"❌ No pitch tracking table found for {player_id}\")\n",
    "#         continue\n",
    "\n",
    "#     # Extract headers in their original order\n",
    "#     header_cells = table.find_all(\"th\")\n",
    "#     headers = [cell.get_text(strip=True) for cell in header_cells]\n",
    "    \n",
    "#     # Store the first table's columns as reference if not already set\n",
    "#     if not reference_columns and len(headers) > 0:\n",
    "#         reference_columns = headers.copy()\n",
    "#         print(f\"📊 Using column order from first found table: {reference_columns}\")\n",
    "\n",
    "#     # Extract rows\n",
    "#     rows_found = 0\n",
    "#     for tr in table.find(\"tbody\").find_all(\"tr\") if table.find(\"tbody\") else table.find_all(\"tr\"):\n",
    "#         cells = tr.find_all(\"td\")\n",
    "#         if not cells or len(cells) <= 1:\n",
    "#             continue\n",
    "            \n",
    "#         # Skip \"Show More Seasons\" rows\n",
    "#         if len(cells) == 1 and \"Show More\" in cells[0].get_text():\n",
    "#             continue\n",
    "            \n",
    "#         # Skip note rows about years in reverse order\n",
    "#         if len(cells) == 1 and \"Note:\" in cells[0].get_text():\n",
    "#             continue\n",
    "\n",
    "#         row_data = {\"Player ID\": player_id}\n",
    "#         for i, cell in enumerate(cells):\n",
    "#             if i < len(headers):\n",
    "#                 span = cell.find(\"span\")\n",
    "#                 text = span.get_text(strip=True) if span else cell.get_text(strip=True)\n",
    "                \n",
    "#                 # Add to row data using original header names\n",
    "#                 row_data[headers[i]] = text\n",
    "\n",
    "#         # Only add rows that have actual data\n",
    "#         if len(row_data) > 1:  # More than just Player ID\n",
    "#             all_rows.append(row_data)\n",
    "#             rows_found += 1\n",
    "\n",
    "#     if rows_found > 0:\n",
    "#         files_with_data += 1\n",
    "#         print(f\"✓ Extracted {rows_found} rows for {player_id}\")\n",
    "#     else:\n",
    "#         print(f\"⚠️ No rows found for {player_id}\")\n",
    "    \n",
    "#     # Save every 20 files\n",
    "#     if files_processed % 20 == 0 and all_rows:\n",
    "#         # Create intermediate save\n",
    "#         temp_df = pd.DataFrame(all_rows)\n",
    "        \n",
    "#         # Clean up numeric values\n",
    "#         for col in temp_df.columns:\n",
    "#             if temp_df[col].dtype == object:  # Only process string columns\n",
    "#                 temp_df[col] = temp_df[col].astype(str).str.replace(\",\", \"\")\n",
    "        \n",
    "#         # Filter out rows where Year is \"Player\" or \"MLB\"\n",
    "#         if \"Year\" in temp_df.columns:\n",
    "#             temp_df = temp_df[~temp_df[\"Year\"].isin([\"Player\", \"MLB\"])]\n",
    "        \n",
    "#         # Force column order to match original table\n",
    "#         cols_to_use = [\"Player ID\"]\n",
    "#         for col in reference_columns:\n",
    "#             if col in temp_df.columns:\n",
    "#                 cols_to_use.append(col)\n",
    "        \n",
    "#         # Add any columns that might be in the data but not in our reference\n",
    "#         for col in temp_df.columns:\n",
    "#             if col not in cols_to_use:\n",
    "#                 cols_to_use.append(col)\n",
    "                \n",
    "#         temp_df = temp_df[cols_to_use]\n",
    "            \n",
    "#         # Save intermediate result\n",
    "#         temp_df.to_csv(output_file, index=False)\n",
    "#         print(f\"💾 Intermediate save: {len(all_rows)} rows saved after processing {files_processed} files\")\n",
    "\n",
    "# print(f\"\\n🏁 Processing complete!\")\n",
    "# print(f\"Files processed: {files_processed}\")\n",
    "# print(f\"Files with pitch tracking data: {files_with_data}\")\n",
    "# print(f\"Total rows collected: {len(all_rows)}\")\n",
    "\n",
    "# if all_rows:\n",
    "#     # Create final DataFrame\n",
    "#     df = pd.DataFrame(all_rows)\n",
    "    \n",
    "#     # Clean up numeric values\n",
    "#     for col in df.columns:\n",
    "#         if df[col].dtype == object:  # Only process string columns\n",
    "#             df[col] = df[col].astype(str).str.replace(\",\", \"\")\n",
    "    \n",
    "#     # Filter out rows where Year is \"Player\" or \"MLB\"\n",
    "#     if \"Year\" in df.columns:\n",
    "#         df = df[~df[\"Year\"].isin([\"Player\", \"MLB\"])]\n",
    "    \n",
    "#     # Force column order to match original table\n",
    "#     cols_to_use = [\"Player ID\"]\n",
    "#     for col in reference_columns:\n",
    "#         if col in df.columns:\n",
    "#             cols_to_use.append(col)\n",
    "    \n",
    "#     # Add any columns that might be in the data but not in our reference\n",
    "#     for col in df.columns:\n",
    "#         if col not in cols_to_use:\n",
    "#             cols_to_use.append(col)\n",
    "    \n",
    "#     # Reorder columns according to our list\n",
    "#     df = df[cols_to_use]\n",
    "    \n",
    "#     # Make sure output directory exists\n",
    "#     output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "#     df.to_csv(output_file, index=False)\n",
    "#     print(f\"💾 Final data saved to {output_file}\")\n",
    "#     print(f\"Final column order: {list(df.columns)}\")\n",
    "    \n",
    "#     if len(df) > 0:\n",
    "#         print(f\"Sample of extracted data:\")\n",
    "#         print(df.head(2))\n",
    "# else:\n",
    "#     print(\"⚠️ No pitch tracking data found to save\")\n",
    "\n",
    "# # import ace_tools as tools; tools.display_dataframe_to_user(name=\"Pitch Tracking Stats\", dataframe=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01a37e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bdef71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ebec88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc836f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bba43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# from bs4 import BeautifulSoup\n",
    "# from pathlib import Path\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# input_folder = Path(\"data/raw/players-batting\")\n",
    "# output_file = Path(\"data/player_pitch_tracking_stats.csv\")\n",
    "\n",
    "# all_rows = []\n",
    "# all_columns = set()\n",
    "\n",
    "# # Loop through all player HTML files\n",
    "# for file_path in tqdm(list(input_folder.glob(\"*.html\")), desc=\"Processing players\"):\n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         soup = BeautifulSoup(f.read(), \"html.parser\")\n",
    "\n",
    "#     player_id = file_path.stem\n",
    "\n",
    "#     # Locate the Pitch Tracking table\n",
    "#     table = soup.find(\"table\", {\"id\": \"detailedPitches\"})\n",
    "#     if not table:\n",
    "#         continue\n",
    "\n",
    "#     # Extract headers\n",
    "#     header_cells = table.find(\"thead\").find_all(\"th\")\n",
    "#     headers = [cell.get_text(strip=True) for cell in header_cells]\n",
    "#     all_columns.update(headers)\n",
    "\n",
    "#     # Extract rows\n",
    "#     for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "#         cells = tr.find_all(\"td\")\n",
    "#         if not cells:\n",
    "#             continue\n",
    "\n",
    "#         row_data = {}\n",
    "#         for i, cell in enumerate(cells):\n",
    "#             # Grab text inside <span> or directly from cell\n",
    "#             span = cell.find(\"span\")\n",
    "#             text = span.get_text(strip=True) if span else cell.get_text(strip=True)\n",
    "#             col_name = headers[i] if i < len(headers) else f\"Extra_{i}\"\n",
    "#             row_data[col_name] = text\n",
    "#             all_columns.add(col_name)\n",
    "\n",
    "#         row_data[\"Player ID\"] = player_id\n",
    "#         all_rows.append(row_data)\n",
    "\n",
    "# # Ensure consistent columns\n",
    "# final_columns = [\"Player ID\"] + sorted(all_columns - {\"Player ID\"})\n",
    "# df = pd.DataFrame(all_rows, columns=final_columns)\n",
    "\n",
    "# # Clean up numeric values (remove commas)\n",
    "# for col in df.columns:\n",
    "#     df[col] = df[col].str.replace(\",\", \"\", regex=False)\n",
    "\n",
    "# df.to_csv(output_file, index=False)\n",
    "# print(f\"✅ Done. Saved pitch tracking stats to: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed14362f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f78e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Pitching Stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c84e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda0705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05501702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
