{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccd5d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from pathlib import Path\n",
    "import re\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import aiohttp\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52e2d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data/\n",
    "!mkdir -p data/raw "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbeecba",
   "metadata": {},
   "source": [
    "# MLB.com v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80167ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import glob\n",
    "from lxml import html\n",
    "from time import sleep\n",
    "from random import uniform\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from pathlib import Path\n",
    "import os\n",
    "os.makedirs(\"data/mlb\", exist_ok=True)\n",
    "os.makedirs(\"data/mlb/depth-charts\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094efd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "### All MLB Team URLs ###\n",
    "url = \"https://www.mlb.com/team/roster/depth-chart\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  \n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "depth_chart_data = []\n",
    "for a in soup.find_all(\"a\", href=True):\n",
    "    href = a[\"href\"]\n",
    "    if \"/roster/depth-chart\" in href:\n",
    "        img = a.find(\"img\")\n",
    "        if img and \"alt\" in img.attrs:\n",
    "            team_name = img[\"alt\"].replace(\" logo\", \"\").strip()\n",
    "            full_url = f\"https://www.mlb.com{href}\" if href.startswith(\"/\") else href\n",
    "            depth_chart_data.append((team_name, full_url))\n",
    "depth_chart_df = pd.DataFrame(depth_chart_data, columns=[\"Team Name\", \"Depth Chart URL\"])\n",
    "depth_chart_df.to_csv(\"data/mlb/all_teams.csv\", index=False)\n",
    "print(\"Saved to all_teams.csv\")\n",
    "print(depth_chart_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674388b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### All Teams Depth Charts ###\n",
    "teams_df = pd.read_csv('data/mlb/all_teams.csv')\n",
    "for index, row in teams_df.iterrows():\n",
    "    team_name = row['Team Name']\n",
    "    depth_chart_url = row['Depth Chart URL']\n",
    "    response = requests.get(depth_chart_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    player_tds = soup.select(\"td.info\")\n",
    "    players_data = []\n",
    "    for td in player_tds:\n",
    "        name_tag = td.find(\"a\")\n",
    "        jersey_tag = td.find(\"span\", class_=\"jersey\")\n",
    "        status_tag = td.find(\"span\", class_=\"status-il\")\n",
    "        mobile_info = td.find(\"div\", class_=\"mobile-info\")\n",
    "        name = name_tag.get_text(strip=True) if name_tag else \"\"\n",
    "        player_url = \"https://www.mlb.com\" + name_tag[\"href\"] if name_tag and name_tag.has_attr(\"href\") else \"\"\n",
    "        jersey = jersey_tag.get_text(strip=True) if jersey_tag else \"\"\n",
    "        status = status_tag.get_text(strip=True) if status_tag else \"\"\n",
    "        bt = mobile_info.find(\"span\", class_=\"mobile-info__bat-throw\").get_text(strip=True).replace(\"B/T: \", \"\") if mobile_info else \"\"\n",
    "        height = mobile_info.find(\"span\", class_=\"mobile-info__height\").get_text(strip=True).replace(\"Ht: \", \"\") if mobile_info else \"\"\n",
    "        weight = mobile_info.find(\"span\", class_=\"mobile-info__weight\").get_text(strip=True).replace(\"Wt: \", \"\") if mobile_info else \"\"\n",
    "        dob = mobile_info.find(\"span\", class_=\"mobile-info__birthday\").get_text(strip=True).replace(\"DOB: \", \"\") if mobile_info else \"\"\n",
    "        players_data.append({\n",
    "            \"Name\": name,\n",
    "            \"Player URL\": player_url,\n",
    "            \"Jersey Number\": jersey,\n",
    "            \"Status\": status,\n",
    "            \"B/T\": bt,\n",
    "            \"Height\": height,\n",
    "            \"Weight\": weight,\n",
    "            \"DOB\": dob\n",
    "        })\n",
    "    players_df = pd.DataFrame(players_data)\n",
    "    csv_path = f\"data/mlb/depth-charts/{team_name.replace(' ', '_').lower()}_depth_chart.csv\"\n",
    "    players_df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved data for {team_name} to {csv_path}\")\n",
    "\n",
    "\n",
    "# import cloudscraper\n",
    "# import pandas as pd\n",
    "# from bs4 import BeautifulSoup\n",
    "# from fake_useragent import UserAgent\n",
    "# import time\n",
    "# import random\n",
    "\n",
    "# # Create scraper instance\n",
    "# scraper = cloudscraper.create_scraper(\n",
    "#     browser={'browser': 'firefox', 'platform': 'darwin', 'mobile': False}\n",
    "# )\n",
    "# ua = UserAgent()\n",
    "\n",
    "# # Read teams\n",
    "# teams_df = pd.read_csv('data/mlb/all_teams.csv')\n",
    "\n",
    "# for index, row in teams_df.iterrows():\n",
    "#     # Random delay between requests (2-7 seconds)\n",
    "#     time.sleep(random.uniform(1, 3))\n",
    "    \n",
    "#     team_name = row['Team Name']\n",
    "#     depth_chart_url = row['Depth Chart URL']\n",
    "    \n",
    "#     # Rotate user agent for each request\n",
    "#     headers = {'User-Agent': ua.random}\n",
    "    \n",
    "#     try:\n",
    "#         response = scraper.get(depth_chart_url, headers=headers)\n",
    "#         soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "#         player_tds = soup.select(\"td.info\")\n",
    "#         players_data = []\n",
    "        \n",
    "#         for td in player_tds:\n",
    "#             name_tag = td.find(\"a\")\n",
    "#             jersey_tag = td.find(\"span\", class_=\"jersey\")\n",
    "#             status_tag = td.find(\"span\", class_=\"status-il\")\n",
    "#             mobile_info = td.find(\"div\", class_=\"mobile-info\")\n",
    "            \n",
    "#             players_data.append({\n",
    "#                 \"Name\": name_tag.get_text(strip=True) if name_tag else \"\",\n",
    "#                 \"Player URL\": \"https://www.mlb.com\" + name_tag[\"href\"] if name_tag and name_tag.has_attr(\"href\") else \"\",\n",
    "#                 \"Jersey Number\": jersey_tag.get_text(strip=True) if jersey_tag else \"\",\n",
    "#                 \"Status\": status_tag.get_text(strip=True) if status_tag else \"\",\n",
    "#                 \"B/T\": mobile_info.find(\"span\", class_=\"mobile-info__bat-throw\").get_text(strip=True).replace(\"B/T: \", \"\") if mobile_info else \"\",\n",
    "#                 \"Height\": mobile_info.find(\"span\", class_=\"mobile-info__height\").get_text(strip=True).replace(\"Ht: \", \"\") if mobile_info else \"\",\n",
    "#                 \"Weight\": mobile_info.find(\"span\", class_=\"mobile-info__weight\").get_text(strip=True).replace(\"Wt: \", \"\") if mobile_info else \"\",\n",
    "#                 \"DOB\": mobile_info.find(\"span\", class_=\"mobile-info__birthday\").get_text(strip=True).replace(\"DOB: \", \"\") if mobile_info else \"\"\n",
    "#             })\n",
    "\n",
    "#         players_df = pd.DataFrame(players_data)\n",
    "#         csv_path = f\"data/mlb/depth-charts/{team_name.replace(' ', '_').lower()}_depth_chart.csv\"\n",
    "#         players_df.to_csv(csv_path, index=False)\n",
    "#         print(f\"Saved data for {team_name} to {csv_path}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error scraping {team_name}: {str(e)}\")\n",
    "#         continue  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794c495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combine all depth chart CSVs into one file and add 'Player ID' column ###\n",
    "all_players = []\n",
    "for team_file in glob.glob(\"data/mlb/depth-charts/*_depth_chart.csv\"):\n",
    "    df = pd.read_csv(team_file)\n",
    "    team_name = team_file.split('/')[-1].replace('_depth_chart.csv','').replace('_',' ').title()\n",
    "    df['Team'] = team_name\n",
    "    df['Player ID'] = df['Player URL'].apply(lambda x: x.split('/')[-1])  # Extracting player ID from URL\n",
    "    all_players.append(df)\n",
    "all_players_df = pd.concat(all_players, ignore_index=True).drop(['Jersey Number', 'Status', 'DOB'], axis=1)\n",
    "all_players_df.to_csv(\"data/mlb/all_players.csv\", index=False)\n",
    "print(f\"Combined {len(all_players)} team rosters into all_players.csv\")\n",
    "print(f\"Total players: {len(all_players_df)}\\n\")\n",
    "print(all_players_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4096d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scrape Player Positions ###\n",
    "\n",
    "# os.makedirs(\"data/mlb/raw\", exist_ok=True)\n",
    "# nest_asyncio.apply()\n",
    "# players_df = pd.read_csv(\"data/mlb/all_players.csv\")\n",
    "# if \"Position\" not in players_df.columns:\n",
    "#     players_df[\"Position\"] = \"\"\n",
    "# async def main():\n",
    "#     async with aiohttp.ClientSession() as session:\n",
    "#         semaphore = asyncio.Semaphore(5) \n",
    "#         tasks = []\n",
    "#         for i, row in players_df.iterrows():\n",
    "#             if pd.notna(row.get(\"Position\")) and row[\"Position\"].strip() != \"\":\n",
    "#                 print(f\"Skipping {row['Player URL']} - already has position: {row['Position']}\")\n",
    "#                 continue\n",
    "#             async def process_player(url, idx):\n",
    "#                 async with semaphore:  \n",
    "#                     print(f\"Scraping {url}...\")\n",
    "#                     try:\n",
    "#                         async with session.get(url, timeout=30, allow_redirects=True) as response:\n",
    "#                         # async with session.get(url, timeout=30) as response:\n",
    "#                             content = await response.text()\n",
    "                            \n",
    "#                             # Save raw HTML using final URL after redirects\n",
    "#                             final_url = str(response.url)\n",
    "#                             safe_filename = final_url.replace(':', '_').replace('/', '_')\n",
    "#                             with open(f\"data/mlb/raw-players/{safe_filename}.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "#                                 f.write(content)\n",
    "                            \n",
    "#                             tree = html.fromstring(content)\n",
    "#                             position = tree.xpath('/html/body/main/section/header/div/div[1]/ul/li[1]/text()')\n",
    "#                             players_df.at[idx, \"Position\"] = position[0].strip() if position else \"\"\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Error for {url}: {e}\")\n",
    "#                     if (idx + 1) % 100 == 0:  \n",
    "#                         players_df.to_csv(\"data/mlb/all_players.csv\", index=False)  \n",
    "#                         print(f\"Checkpoint saved at row {idx+1}\")\n",
    "#                     await asyncio.sleep(uniform(1.5, 3.0))\n",
    "#             tasks.append(process_player(row[\"Player URL\"], i))\n",
    "#         if not tasks:\n",
    "#             print(\"No players need position information. All done!\")\n",
    "#             return\n",
    "#         print(f\"Need to scrape {len(tasks)} players for position data\")\n",
    "#         await asyncio.gather(*tasks)\n",
    "# asyncio.run(main())\n",
    "# players_df.to_csv(\"data/mlb/all_players.csv\", index=False)\n",
    "# print(\"Saved to all_players.csv\")\n",
    "\n",
    "\n",
    "\n",
    "### Scrape Player Positions ###\n",
    "nest_asyncio.apply()\n",
    "players_df = pd.read_csv(\"data/mlb/all_players.csv\")\n",
    "if \"Position\" not in players_df.columns:\n",
    "    players_df[\"Position\"] = \"\"\n",
    "# if \"Raw URL\" not in players_df.columns:\n",
    "#     players_df[\"Raw URL\"] = \"\"\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        semaphore = asyncio.Semaphore(10) \n",
    "        tasks = []\n",
    "        for i, row in players_df.iterrows():\n",
    "            if pd.notna(row.get(\"Position\")) and row[\"Position\"].strip() != \"\":\n",
    "                print(f\"Skipping {row['Player URL']} - already has position: {row['Position']}\")\n",
    "                continue\n",
    "            # if pd.notna(row.get(\"Position\")) and row[\"Position\"].strip() != \"\" and pd.notna(row.get(\"Raw URL\")) and row[\"Raw URL\"].strip() != \"\":\n",
    "            #     print(f\"Skipping {row['Player URL']} - already has position: {row['Position']} and Raw URL\")\n",
    "            #     continue\n",
    "            async def process_player(url, idx):\n",
    "                async with semaphore:  \n",
    "                    print(f\"Scraping {url}...\")\n",
    "                    try:\n",
    "                        # async with session.get(url, timeout=10) as response:\n",
    "                        async with session.get(url, timeout=10, allow_redirects=True) as response:\n",
    "                            # Save the final URL after any redirects\n",
    "                            # final_url = str(response.url)\n",
    "                            # players_df.at[idx, \"Raw URL\"] = final_url\n",
    "                            \n",
    "                            content = await response.text()\n",
    "                            tree = html.fromstring(content)\n",
    "                            position = tree.xpath('/html/body/main/section/header/div/div[1]/ul/li[1]/text()')\n",
    "                            players_df.at[idx, \"Position\"] = position[0].strip() if position else \"\"\n",
    "                            \n",
    "                            print(f\"  Position: {players_df.at[idx, 'Position']}\")\n",
    "                            # print(f\"  Raw URL: {final_url}\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error for {url}: {e}\")\n",
    "                    if (idx + 1) % 100 == 0:  \n",
    "                        players_df.to_csv(\"data/mlb/all_players.csv\", index=False)  \n",
    "                        print(f\"Checkpoint saved at row {idx+1}\")\n",
    "                    await asyncio.sleep(uniform(1.5, 3.0))\n",
    "            tasks.append(process_player(row[\"Player URL\"], i))\n",
    "        if not tasks:\n",
    "            print(\"No players need position information. All done!\")\n",
    "            return\n",
    "        print(f\"Need to scrape {len(tasks)} players for position data\")\n",
    "        # print(f\"Need to scrape {len(tasks)} players for position data and/or Raw URL\")\n",
    "        await asyncio.gather(*tasks)\n",
    "asyncio.run(main())\n",
    "players_df.to_csv(\"data/mlb/all_players.csv\", index=False)\n",
    "print(\"Saved to all_players.csv\")\n",
    "\n",
    "# players_df = pd.read_csv(\"data/mlb/all_players.csv\")  \n",
    "# players_df[\"Position\"] = \"\"\n",
    "# for i, row in players_df.iterrows():\n",
    "#     url = row[\"Player URL\"]\n",
    "#     print(f\"Scraping {url}...\")\n",
    "#     try:\n",
    "#         response = requests.get(url, timeout=20)\n",
    "#         response.raise_for_status()\n",
    "#         tree = html.fromstring(response.content)\n",
    "#         position = tree.xpath('/html/body/main/section/header/div/div[1]/ul/li[1]/text()')\n",
    "#         players_df.at[i, \"Position\"] = position[0].strip() if position else \"\"\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error for {url}: {e}\")\n",
    "#     sleep(uniform(1.5, 3.0))\n",
    "    \n",
    "#     if (i + 1) % 100 == 0:     \n",
    "#         players_df.to_csv(\"data/all_players_with_positions.csv\", index=False)\n",
    "#         print(f\"Checkpoint saved at row {i+1}\")\n",
    "\n",
    "# players_df.to_csv(\"data/mlb/all_players_with_positions.csv\", index=False)\n",
    "# print(\"Saved to all_players_with_positions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0d4b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter pitchers and non-pitchers and generate game log URLs ###\n",
    "players_df = pd.read_csv(\"data/mlb/all_players.csv\")\n",
    "non_pitchers_df = players_df[\n",
    "    players_df[\"Position\"].notna() &\n",
    "    (players_df[\"Position\"].str.strip().str.upper() != \"P\")\n",
    "].copy()\n",
    "non_pitchers_df.loc[:, \"Game Log URL\"] = non_pitchers_df[\"Player ID\"].apply(\n",
    "    lambda pid: f\"https://www.mlb.com/player/{pid}?stats=gamelogs-r-hitting-mlb&year=2025\"\n",
    ")\n",
    "non_pitchers_df.to_csv(\"data/mlb/non_pitchers_with_game_logs.csv\", index=False)\n",
    "print(f\"Saved {len(non_pitchers_df)} non-pitchers with game log URLs to data/mlb/non_pitchers_with_game_logs.csv\")\n",
    "pitchers_df = players_df[\n",
    "    players_df[\"Position\"].notna() &\n",
    "    (players_df[\"Position\"].str.strip().str.upper() == \"P\")\n",
    "].copy()\n",
    "pitchers_df.loc[:, \"Game Log URL\"] = pitchers_df[\"Player ID\"].apply(\n",
    "    lambda pid: f\"https://www.mlb.com/player/{pid}?stats=gamelogs-r-pitching-mlb&year=2025\"\n",
    ")\n",
    "pitchers_df.to_csv(\"data/mlb/pitchers_with_game_logs.csv\", index=False)\n",
    "print(f\"Saved {len(pitchers_df)} pitchers with game log URLs to data/mlb/pitchers_with_game_logs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af19457",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scrape non-pitchers 2025 game logs ###\n",
    "# from playwright.async_api import async_playwright\n",
    "# import pandas as pd\n",
    "# import asyncio\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "# players_df = pd.read_csv(\"data/mlb/non_pitchers_with_game_logs.csv\")\n",
    "# output_dir = \"data/mlb/raw_player_game_logs\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# semaphore = asyncio.Semaphore(10)\n",
    "# async def save_page(url, player_id, pbar):\n",
    "#     output_file = os.path.join(output_dir, f\"{player_id}_2025.html\")\n",
    "#     if os.path.exists(output_file):\n",
    "#         pbar.update(1)\n",
    "#         return\n",
    "#     async with semaphore:\n",
    "#         async with async_playwright() as p:\n",
    "#             browser = await p.chromium.launch(headless=True)\n",
    "#             page = await browser.new_page()\n",
    "#             try:\n",
    "#                 await page.goto(url)\n",
    "#                 # await page.goto(url, timeout=20000)\n",
    "#                 # await page.wait_for_timeout(5000)  # wait for JS to load\n",
    "#                 content = await page.content()\n",
    "#                 with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#                     f.write(content)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Failed to load {url}: {e}\")\n",
    "#             await browser.close()\n",
    "#             pbar.update(1)\n",
    "# async def main():\n",
    "#     tasks = []\n",
    "#     pbar = tqdm(total=len(players_df), desc=\"Downloading game logs\")\n",
    "#     for _, row in players_df.iterrows():\n",
    "#         tasks.append(save_page(row[\"Game Log URL\"], row[\"Player ID\"], pbar))\n",
    "#     await asyncio.gather(*tasks)\n",
    "#     pbar.close()\n",
    "# asyncio.run(main())\n",
    "\n",
    "\n",
    "# Selenium version ^\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "players_df = pd.read_csv(\"data/mlb/non_pitchers_with_game_logs.csv\")\n",
    "output_dir = \"data/mlb/raw_player_game_logs_selenium\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "def save_page(url, player_id):\n",
    "    output_file = os.path.join(output_dir, f\"{player_id}_2025.html\")\n",
    "    if os.path.exists(output_file):\n",
    "        return\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(url)\n",
    "        content = driver.page_source\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {url}: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "def main():\n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        list(tqdm(\n",
    "            executor.map(lambda row: save_page(row[1][\"Game Log URL\"], row[1][\"Player ID\"]), players_df.iterrows()),\n",
    "            total=len(players_df),\n",
    "            desc=\"Downloading game logs\"\n",
    "        ))\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf49a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parse non-pitchers 2025 game logs ###\n",
    "from lxml import html\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "input_dir = \"data/mlb/raw_player_game_logs_selenium\"\n",
    "output_dir = \"data/mlb/player_game_logs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "all_files = Path(input_dir).glob(\"*.html\")\n",
    "for file_path in all_files:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        tree = html.fromstring(f.read())\n",
    "    header_xpath = \"/html/body/main/section/section/section[2]/div/section/div[3]/div[2]/div/div[1]/div/div/div[1]/div/table/thead/tr\"\n",
    "    row_xpath = \"/html/body/main/section/section/section[2]/div/section/div[3]/div[2]/div/div[1]/div/div/div[1]/div/table/tbody/tr\"\n",
    "    header_el = tree.xpath(header_xpath)\n",
    "    headers = [th.text_content().strip() for th in header_el[0].xpath(\".//th\")] if header_el else []\n",
    "    rows = tree.xpath(row_xpath)\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        values = [td.text_content().strip() for td in tr.xpath(\".//td\")]\n",
    "        if values:\n",
    "            data.append(values)\n",
    "    if data:\n",
    "        df = pd.DataFrame(data, columns=headers if headers else None)\n",
    "        player_id = file_path.stem.replace(\"_2025\", \"\")\n",
    "        df.to_csv(f\"{output_dir}/{player_id}_2025.csv\", index=False)\n",
    "        print(f\"Saved: {output_dir}/{player_id}_2025.csv\")\n",
    "    else:\n",
    "        print(f\"No valid game data found in {file_path.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7b4447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ce13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7c88b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03942122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED ^\n",
    "\n",
    "# import pandas as pd\n",
    "# import aiohttp\n",
    "# import asyncio\n",
    "# from lxml import html\n",
    "# from random import uniform\n",
    "# import nest_asyncio\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "\n",
    "# ### Scrape Player Game Logs ###\n",
    "\n",
    "# nest_asyncio.apply()\n",
    "# players_df = pd.read_csv(\"data/mlb/all_players.csv\")\n",
    "# non_pitchers_df = players_df[\n",
    "#     players_df[\"Position\"].notna() &\n",
    "#     (players_df[\"Position\"].str.strip().str.upper() != \"P\")\n",
    "# ].copy()\n",
    "# non_pitchers_df.loc[:, \"Game Log URL\"] = non_pitchers_df[\"Player ID\"].apply(\n",
    "#     lambda pid: f\"https://www.mlb.com/player/{pid}?stats=gamelogs-r-hitting-mlb&year=2025\"\n",
    "# )\n",
    "\n",
    "# # Create output directory\n",
    "# output_dir = Path(\"data/mlb/player_game_logs\")\n",
    "# output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# async def main():\n",
    "#     async with aiohttp.ClientSession() as session:\n",
    "#         semaphore = asyncio.Semaphore(18)  # Stay under 20/min limit\n",
    "#         tasks = []\n",
    "#         for i, row in non_pitchers_df.iterrows():\n",
    "#             player_id = row[\"Player ID\"]\n",
    "#             output_file = output_dir / f\"{player_id}_game_logs.csv\"\n",
    "            \n",
    "#             # Skip if already processed\n",
    "#             if output_file.exists():\n",
    "#                 print(f\"Skipping {player_id} - already processed\")\n",
    "#                 continue\n",
    "                \n",
    "#             async def process_player(url, player_id, idx):\n",
    "#                 async with semaphore:\n",
    "#                     player_name = row.get(\"Name\", row.get(\"Player Name\", player_id))\n",
    "#                     print(f\"Scraping game logs for {player_name} ({player_id})...\")\n",
    "#                     try:\n",
    "#                         async with session.get(url, timeout=15) as response:\n",
    "#                             content = await response.text()\n",
    "#                             tree = html.fromstring(content)\n",
    "                            \n",
    "#                             # Get header columns\n",
    "#                             header_xpath = \"/html/body/main/section/section/section[2]/div/section/div[3]/div[2]/div/div[1]/div/div/div[1]/div/table/thead/tr\"\n",
    "#                             header_cells = tree.xpath(f\"{header_xpath}/th\")\n",
    "                            \n",
    "#                             if not header_cells:\n",
    "#                                 print(f\"No header cells found for {player_name}\")\n",
    "#                                 return\n",
    "                                \n",
    "#                             headers = [cell.text_content().strip() for cell in header_cells]\n",
    "                            \n",
    "#                             # Get game log rows\n",
    "#                             rows_xpath = \"/html/body/main/section/section/section[2]/div/section/div[3]/div[2]/div/div[1]/div/div/div[1]/div/table/tbody/tr\"\n",
    "#                             row_elements = tree.xpath(rows_xpath)\n",
    "                            \n",
    "#                             if not row_elements:\n",
    "#                                 print(f\"No game log rows found for {player_name}\")\n",
    "#                                 return\n",
    "                                \n",
    "#                             game_logs = []\n",
    "#                             for row_elem in row_elements:\n",
    "#                                 cells = row_elem.xpath(\"./td\")\n",
    "#                                 if cells:\n",
    "#                                     row_data = [cell.text_content().strip() for cell in cells]\n",
    "#                                     game_logs.append(row_data)\n",
    "                            \n",
    "#                             # Create DataFrame and save to CSV\n",
    "#                             if game_logs:\n",
    "#                                 game_logs_df = pd.DataFrame(game_logs, columns=headers)\n",
    "#                                 game_logs_df[\"Player ID\"] = player_id\n",
    "#                                 game_logs_df[\"Player Name\"] = player_name\n",
    "#                                 game_logs_df.to_csv(output_file, index=False)\n",
    "#                                 print(f\"Saved {len(game_logs)} game logs for {player_name}\")\n",
    "#                             else:\n",
    "#                                 print(f\"No game logs data for {player_name}\")\n",
    "                                \n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Error for {player_id}: {e}\")\n",
    "                        \n",
    "#                     # Save checkpoint periodically\n",
    "#                     if (idx + 1) % 50 == 0:\n",
    "#                         print(f\"Checkpoint reached at row {idx+1}\")\n",
    "                        \n",
    "#                     # Rate limiting\n",
    "#                     await asyncio.sleep(uniform(2.5, 3.5))\n",
    "                    \n",
    "#             tasks.append(process_player(row[\"Game Log URL\"], player_id, i))\n",
    "            \n",
    "#         if not tasks:\n",
    "#             print(\"No players need game logs. All done!\")\n",
    "#             return\n",
    "            \n",
    "#         print(f\"Need to scrape game logs for {len(tasks)} players\")\n",
    "#         await asyncio.gather(*tasks)\n",
    "\n",
    "# asyncio.run(main())\n",
    "# print(\"All player game logs saved to data/mlb/player_game_logs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db11341b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c765014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75da03b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac8e02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce165ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from lxml import html\n",
    "from random import uniform\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "players_df = pd.read_csv(\"data/all_players.csv\")\n",
    "players_df[\"Position\"] = \"\"\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        semaphore = asyncio.Semaphore(10)  \n",
    "        tasks = []\n",
    "        for i, row in players_df.iterrows():\n",
    "            async def process_player(url, idx):\n",
    "                async with semaphore:  \n",
    "                    print(f\"Scraping {url}...\")\n",
    "                    try:\n",
    "                        async with session.get(url, timeout=10) as response:\n",
    "                            content = await response.text()\n",
    "                            tree = html.fromstring(content)\n",
    "                            position = tree.xpath('/html/body/main/section/header/div/div[1]/ul/li[1]/text()')\n",
    "                            players_df.at[idx, \"Position\"] = position[0].strip() if position else \"\"\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error for {url}: {e}\")\n",
    "                    if (idx + 1) % 100 == 0:\n",
    "                        players_df.to_csv(\"data/all_players_with_positions.csv\", index=False)\n",
    "                        print(f\"Checkpoint saved at row {idx+1}\")\n",
    "                    await asyncio.sleep(uniform(1.5, 3.0))\n",
    "            tasks.append(process_player(row[\"Player URL\"], i))\n",
    "        await asyncio.gather(*tasks)\n",
    "asyncio.run(main())\n",
    "players_df.to_csv(\"data/all_players_with_positions.csv\", index=False)\n",
    "print(\"Saved to all_players_with_positions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dd0ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbece72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24926595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dda13e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c2365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from time import sleep\n",
    "# from random import uniform\n",
    "\n",
    "# # Load your CSV of player profile URLs\n",
    "# players_df = pd.read_csv(\"data/all_players.csv\")  # This file must have a column: 'Player URL'\n",
    "# players_df[\"Position\"] = \"\"\n",
    "\n",
    "# def extract_position(url):\n",
    "#     try:\n",
    "#         response = requests.get(url, timeout=10)\n",
    "#         response.raise_for_status()\n",
    "#         soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "#         # Look for <ul> elements with <li> children that include common tags like B/T, height, age\n",
    "#         for ul in soup.find_all(\"ul\"):\n",
    "#             lis = ul.find_all(\"li\")\n",
    "#             li_texts = [li.get_text(strip=True) for li in lis]\n",
    "#             if any(\"B/T:\" in t or \"Age:\" in t or \"/\" in t for t in li_texts):\n",
    "#                 return li_texts[0]  # First one is usually the position\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error for {url}: {e}\")\n",
    "#     return \"\"\n",
    "\n",
    "# # Loop through and update positions\n",
    "# for i, row in players_df.iterrows():\n",
    "#     url = row[\"Player URL\"]\n",
    "#     print(f\"Scraping {url}...\")\n",
    "#     position = extract_position(url)\n",
    "#     players_df.at[i, \"Position\"] = position\n",
    "#     sleep(uniform(1.5, 3.0))  # delay to avoid hammering MLB servers\n",
    "\n",
    "# # Save results\n",
    "# players_df.to_csv(\"all_players_with_positions.csv\", index=False)\n",
    "# print(\"Saved to all_players_with_positions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5e6202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0abe2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5413f979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61262535",
   "metadata": {},
   "source": [
    "# MLB.com + baseballsavant.com v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d969b731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Players Name/IDs from https://www.mlb.com/players\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "url = \"https://www.mlb.com/players\"\n",
    "driver.get(url)\n",
    "time.sleep(10)\n",
    "html = driver.page_source\n",
    "output_path = \"data/mlb_players_raw.html\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html)\n",
    "driver.quit()\n",
    "print(\"Downloaded full HTML with JavaScript saved to\", output_path)\n",
    "\n",
    "# Parse All Player ID's\n",
    "html_path = \"data/mlb_players_raw.html\"\n",
    "output_csv = \"data/mlb_players.csv\"\n",
    "with open(html_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    soup = BeautifulSoup(f, \"html.parser\")\n",
    "pattern = re.compile(r\"^/player/([a-z0-9\\-]+-\\d+)$\", re.IGNORECASE)\n",
    "anchors = soup.find_all(\"a\", class_=\"p-related-links__link\")\n",
    "player_entries = []\n",
    "for a in anchors:\n",
    "    href = a.get(\"href\", \"\")\n",
    "    text = a.get_text(strip=True)\n",
    "    match = pattern.match(href)\n",
    "    if match:\n",
    "        raw_slug = match.group(1)\n",
    "        player_entries.append([raw_slug, text])\n",
    "unique = {slug: name for slug, name in player_entries}\n",
    "unique_entries = [[slug, name] for slug, name in unique.items()]\n",
    "with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Player ID\", \"Player Name\"])\n",
    "    writer.writerows(unique_entries)\n",
    "df = pd.read_csv(output_csv)\n",
    "print(\"Sample of extracted player data:\")\n",
    "print(df.head(20))\n",
    "print(f\"Total unique players extracted: {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eca5bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Web Version Directly ^\n",
    "\n",
    "# url = \"https://www.mlb.com/players\"\n",
    "# output_csv = \"data/mlb_players.csv\"\n",
    "\n",
    "# chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")\n",
    "# chrome_options.add_argument(\"--disable-gpu\")\n",
    "# # chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "# driver = webdriver.Chrome(options=chrome_options)\n",
    "# print(\"Loading the page...\")\n",
    "# driver.get(url)\n",
    "# time.sleep(10)\n",
    "# driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "# time.sleep(5)\n",
    "# print(\"Extracting player link elements...\")\n",
    "\n",
    "# player_elements = driver.find_elements(By.CSS_SELECTOR, \"a.p-related-links__link\")\n",
    "# pattern = re.compile(r\"/player/([a-z0-9\\-]+-\\d+)\", re.IGNORECASE)\n",
    "# player_data = []\n",
    "# for elem in player_elements:\n",
    "#     href = elem.get_attribute(\"href\")\n",
    "#     text = elem.text.strip()\n",
    "#     match = pattern.search(href)\n",
    "#     if match:\n",
    "#         raw_slug = match.group(1)\n",
    "#         player_data.append([raw_slug, text])\n",
    "# print(f\"Found {len(player_data)} player entries.\")\n",
    "# driver.quit()  # Close the browser\n",
    "\n",
    "# # --- Write the extracted data to CSV ---\n",
    "# print(f\"Writing data to {output_csv}...\")\n",
    "# with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow([\"Player ID\", \"Player Name\"])\n",
    "#     writer.writerows(player_data)\n",
    "\n",
    "# # --- Preview the results using pandas ---\n",
    "# df = pd.read_csv(output_csv)\n",
    "# print(\"Sample of extracted player data:\")\n",
    "# print(df.head(20))\n",
    "# print(f\"Total unique players extracted: {df.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71319ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download Player Batting Pages\n",
    "# From baseballsavant.mlb.com/savant-player<player-id>?stats=statcast-r-hitting-mlb\n",
    "\n",
    "# Download player batting pages\n",
    "nest_asyncio.apply()\n",
    "csv_path = \"data/mlb_players.csv\"\n",
    "output_folder = Path(\"data/raw/players-batting\")\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "df = pd.read_csv(csv_path)\n",
    "async def fetch(session, url, raw_slug, player_name, max_retries=3):\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            print(f\"Fetching: {player_name} -> {url} (Attempt {attempt})\")\n",
    "            async with session.get(url, timeout=15) as response:\n",
    "                if response.status == 200:\n",
    "                    text = await response.text()\n",
    "                    output_file = output_folder / f\"{raw_slug}.html\"\n",
    "                    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                        f.write(text)\n",
    "                    print(f\"Successfully fetched {player_name}\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"⚠️ Failed: {player_name} (HTTP {response.status}).\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error fetching {url} on attempt {attempt}: {e}\")\n",
    "        if attempt < max_retries:\n",
    "            await asyncio.sleep(1)  # Wait a bit before retrying\n",
    "        else:\n",
    "            print(f\"❌ Giving up on {player_name} after {max_retries} attempts.\")\n",
    "\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for _, row in df.iterrows():\n",
    "            raw_slug = row[\"Player ID\"]     # e.g., \"mike-yastrzemski-573262\"\n",
    "            player_name = row[\"Player Name\"]\n",
    "            url = f\"https://baseballsavant.mlb.com/savant-player/{raw_slug}?stats=statcast-r-hitting-mlb\"\n",
    "            tasks.append(fetch(session, url, raw_slug, player_name))\n",
    "        await asyncio.gather(*tasks)\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "    \n",
    "# v2 #\n",
    "# csv_path = \"data/mlb_players.csv\"\n",
    "# output_folder = Path(\"data/raw/players-batting\")\n",
    "# output_folder.mkdir(parents=True, exist_ok=True)\n",
    "# df = pd.read_csv(csv_path)\n",
    "# options = Options()\n",
    "# options.add_argument(\"--headless\")\n",
    "# options.add_argument(\"--disable-gpu\")\n",
    "# driver = webdriver.Chrome(options=options)\n",
    "# skipped = 0\n",
    "# downloaded = 0\n",
    "# for _, row in df.iterrows():\n",
    "#     raw_slug = row[\"Player ID\"]\n",
    "#     player_name = row[\"Player Name\"]\n",
    "#     output_file = output_folder / f\"{raw_slug}.html\"\n",
    "#     if output_file.exists():\n",
    "#         print(f\"Skipping: {player_name} (already downloaded)\")\n",
    "#         skipped += 1\n",
    "#         continue\n",
    "        \n",
    "#     url = f\"https://baseballsavant.mlb.com/savant-player/{raw_slug}?stats=statcast-r-hitting-mlb\"\n",
    "#     try:\n",
    "#         print(f\"Fetching: {player_name} -> {url}\")\n",
    "#         driver.get(url)\n",
    "#         html = driver.page_source\n",
    "#         with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#             f.write(html)\n",
    "#         downloaded += 1\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error fetching {url}: {e}\")\n",
    "# driver.quit()\n",
    "# print(f\"Completed downloading {len(df)} player pages to {output_folder}\")\n",
    "\n",
    "# v0 #\n",
    "# csv_path = \"data/mlb_players.csv\"\n",
    "# output_folder = Path(\"data/raw/players-batting\")\n",
    "# output_folder.mkdir(parents=True, exist_ok=True)\n",
    "# df = pd.read_csv(csv_path)\n",
    "# for idx, row in df.iterrows():\n",
    "#     raw_slug = row[\"Player ID\"]  # e.g., \"mike-yastrzemski-573262\"\n",
    "#     player_name = row[\"Player Name\"]\n",
    "\n",
    "#     url = f\"https://baseballsavant.mlb.com/savant-player/{raw_slug}?stats=statcast-r-hitting-mlb\"\n",
    "#     print(f\"[{idx+1}/{len(df)}] Fetching: {player_name} -> {url}\")\n",
    "\n",
    "#     try:\n",
    "#         response = requests.get(url, timeout=15)\n",
    "#         if response.status_code == 200:\n",
    "#             output_file = output_folder / f\"{raw_slug}.html\"\n",
    "#             with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#                 f.write(response.text)\n",
    "#         else:\n",
    "#             print(f\"⚠️ Failed: {player_name} (HTTP {response.status_code})\")\n",
    "#     except requests.RequestException as e:\n",
    "#         print(f\"❌ Error fetching {url}: {e}\")\n",
    "#     time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe23629",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parse For General Player Info ###\n",
    "\n",
    "input_folder = Path(\"data/raw/players-batting\")\n",
    "output_csv = \"data/player_general_info.csv\"\n",
    "rows = []\n",
    "all_files = list(input_folder.glob(\"*.html\"))\n",
    "total_files = len(all_files)\n",
    "processed = 0\n",
    "print(f\"Processing {total_files} player files...\")\n",
    "\n",
    "# Change the loop to use all_files\n",
    "# for file in input_folder.glob(\"*.html\"):\n",
    "for file in all_files:\n",
    "    processed += 1\n",
    "    if processed % 20 == 0 or processed == total_files:\n",
    "        print(f\"Progress: {processed}/{total_files} files ({processed/total_files*100:.1f}%)\")\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "    bio_section = soup.find(\"div\", class_=\"bio-player-name\")\n",
    "    if not bio_section:\n",
    "        continue\n",
    "    player_name = bio_section.find(\"div\").get_text(strip=True)\n",
    "    subsections = bio_section.find_all(\"div\", style=lambda v: v and \"font-size: .8rem\" in v)\n",
    "    if len(subsections) < 1:\n",
    "        continue\n",
    "    line1 = subsections[0].get_text(\" \", strip=True)\n",
    "    parts = [p.strip() for p in line1.split(\"|\")]\n",
    "    if len(parts) < 4:\n",
    "        continue\n",
    "    position = parts[0]\n",
    "    bats_throws = parts[1].replace(\"Bats/Throws:\", \"\").strip()\n",
    "    height_weight = parts[2].strip()\n",
    "    age = parts[3].replace(\"Age:\", \"\").strip()\n",
    "    hw_parts = height_weight.split()\n",
    "    height = hw_parts[0] + \" \" + hw_parts[1]\n",
    "    weight = hw_parts[2] if len(hw_parts) > 2 else \"\"\n",
    "    player_id = file.stem\n",
    "    rows.append({\n",
    "        \"Player ID\": player_id,\n",
    "        \"Position\": position,\n",
    "        \"Bats/Throws\": bats_throws,\n",
    "        \"Height\": height,\n",
    "        \"Weight\": weight,\n",
    "        \"Age\": age\n",
    "    })\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ Saved {len(df)} players to {output_csv}\")\n",
    "\n",
    "!open data/player_general_info.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b08372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Player Pitching Pages\n",
    "# From https://baseballsavant.mlb.com/savant-player/shohei-ohtani-660271?stats=statcast-r-pitching-mlb&playerType=pitcher\n",
    "\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Download player pitching pages\n",
    "nest_asyncio.apply()\n",
    "csv_path = \"data/player_general_info.csv\"\n",
    "output_folder = Path(\"data/raw/players-pitching\")\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df[df['Position'].str.contains('P')]  # Filter to only include pitchers\n",
    "\n",
    "async def fetch(session, url, raw_slug, player_name, max_retries=3):\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            print(f\"Fetching: {player_name} -> {url} (Attempt {attempt})\")\n",
    "            async with session.get(url, timeout=15) as response:\n",
    "                if response.status == 200:\n",
    "                    text = await response.text()\n",
    "                    output_file = output_folder / f\"{raw_slug}.html\"\n",
    "                    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                        f.write(text)\n",
    "                    print(f\"Successfully fetched {player_name}\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"⚠️ Failed: {player_name} (HTTP {response.status}).\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error fetching {url} on attempt {attempt}: {e}\")\n",
    "        if attempt < max_retries:\n",
    "            await asyncio.sleep(1)  # Wait a bit before retrying\n",
    "        else:\n",
    "            print(f\"❌ Giving up on {player_name} after {max_retries} attempts.\")\n",
    "\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for _, row in df.iterrows():\n",
    "            raw_slug = row[\"Player ID\"]     # e.g., \"shohei-ohtani-660271\"\n",
    "            # player_name = row[\"Player Name\"]\n",
    "            url = f\"https://baseballsavant.mlb.com/savant-player/{raw_slug}?stats=statcast-r-pitching-mlb&playerType=pitcher\"\n",
    "            tasks.append(fetch(session, url, raw_slug, player_name))\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2578114",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parse for Batting Statistics Table ###\n",
    "\n",
    "input_folder = Path(\"data/raw/players-batting\")\n",
    "output_csv = \"data/player_batting_stats.csv\"\n",
    "all_rows = []\n",
    "\n",
    "all_files = list(input_folder.glob(\"*.html\"))\n",
    "total_files = len(all_files)\n",
    "processed = 0\n",
    "success = 0\n",
    "print(f\"Processing {total_files} player files for batting stats...\")\n",
    "for file_path in all_files:\n",
    "    processed += 1\n",
    "    if processed % 20 == 0 or processed == total_files:\n",
    "        print(f\"Progress: {processed}/{total_files} files ({processed/total_files*100:.1f}%)\")\n",
    "        \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "    slug = file_path.stem\n",
    "    table_div = soup.find(\"div\", id=\"statcast_glance_batter\")\n",
    "    if not table_div:\n",
    "        continue\n",
    "    table = table_div.find(\"table\")\n",
    "    if not table:\n",
    "        continue\n",
    "    headers = [th.get_text(strip=True).replace(\"\\n\", \" \") for th in table.find(\"thead\").find_all(\"th\")]\n",
    "    for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "        cells = tr.find_all(\"td\")\n",
    "        row_data = [td.get_text(strip=True) for td in cells]\n",
    "        if len(row_data) == len(headers):\n",
    "            row_dict = dict(zip(headers, row_data))\n",
    "            row_dict[\"Player ID\"] = slug\n",
    "            all_rows.append(row_dict)\n",
    "            success += 1\n",
    "\n",
    "df = pd.DataFrame(all_rows)\n",
    "df = df[~df['Season'].isin(['Player', 'MLB'])]\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ Saved {len(df)} rows to {output_csv}\")\n",
    "\n",
    "!open data/player_batting_stats.csv\n",
    "\n",
    "# for file_path in input_folder.glob(\"*.html\"):\n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         soup = BeautifulSoup(f, \"html.parser\")\n",
    "#     slug = file_path.stem\n",
    "#     table_div = soup.find(\"div\", id=\"statcast_glance_batter\")\n",
    "#     if not table_div:\n",
    "#         continue\n",
    "#     table = table_div.find(\"table\")\n",
    "#     if not table:\n",
    "#         continue\n",
    "#     headers = [th.get_text(strip=True).replace(\"\\n\", \" \") for th in table.find(\"thead\").find_all(\"th\")]\n",
    "#     for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "#         cells = tr.find_all(\"td\")\n",
    "#         row_data = [td.get_text(strip=True) for td in cells]\n",
    "#         if len(row_data) == len(headers):\n",
    "#             row_dict = dict(zip(headers, row_data))\n",
    "#             row_dict[\"Player Slug\"] = slug\n",
    "#             all_rows.append(row_dict)\n",
    "# df = pd.DataFrame(all_rows)\n",
    "# df.to_csv(output_csv, index=False)\n",
    "# print(f\"✅ Saved {len(df)} rows to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8414d191",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parse for Pitch Tracking Table ###\n",
    "\n",
    "# start:\n",
    "# /html/body/div[2]/section/div/section/div[5]/section/div[1]/div[6]/div[2]/div/table/thead\n",
    "# /html/body/div[2]/section/div/section/div[5]/section/div[1]/div[6]/div[2]/div/table/thead/tr\n",
    "\n",
    "# column 1:\n",
    "# /html/body/div[2]/section/div/section/div[5]/section/div[1]/div[6]/div[2]/div/table/thead/tr/th[1]\n",
    "\n",
    "# columns 2:\n",
    "# /html/body/div[2]/section/div/section/div[5]/section/div[1]/div[6]/div[2]/div/table/thead/tr/th[2]\n",
    "\n",
    "# body:\n",
    "# /html/body/div[2]/section/div/section/div[5]/section/div[1]/div[6]/div[2]/div/table/tbody\n",
    "\n",
    "# row 1 values inside:\n",
    "# /html/body/div[2]/section/div/section/div[5]/section/div[1]/div[6]/div[2]/div/table/tbody/tr[1]\n",
    "\n",
    "# column 1 value\n",
    "# /html/body/div[2]/section/div/section/div[5]/section/div[1]/div[6]/div[2]/div/table/thead/tr/th[1]\n",
    "# /html/body/div[2]/section/div/section/div[5]/section/div[1]/div[6]/div[2]/div/table/thead/tr/th[1]/div\n",
    "\n",
    "# last column value\n",
    "# /html/body/div[2]/section/div/section/div[5]/section/div[1]/div[6]/div[2]/div/table/thead/tr/th[23]\n",
    "# /html/body/div[2]/section/div/section/div[5]/section/div[1]/div[6]/div[2]/div/table/thead/tr/th[23]/div\n",
    "\n",
    "\n",
    "# that is not the correct table for the last fucking time. This Pitch Tracking one is with these columns in the image ONLY \n",
    "# Year, Pitch Type, #, %, PA, AB, H, 1B, 2B, 3B, HR, SO, BBE, BA, XBA, SLG, XSLG, WOBA, XWOBA, EV, LA, Whiff%, PutAway%\n",
    "\n",
    "# Pitch Tracking\n",
    "# 2025\tFastball\t144\t49.0\t39\t36\t11\t8\t0\t0\t3\t5\t31\t.306\t.344\t.556\t.752\t.399\t.477\t95.9\t12\t15.9\t17.9\n",
    "# 2025\tBreaking\t104\t35.4\t25\t21\t5\t3\t1\t1\t0\t9\t12\t.238\t.290\t.381\t.449\t.335\t.378\t97.3\t10\t39.2\t22.0\n",
    "# 2025\tOffspeed\t46\t15.6\t11\t9\t2\t1\t0\t0\t1\t4\t5\t.222\t.280\t.556\t.550\t.399\t.413\t97.3\t7\t42.9\t33.3\n",
    "# 2024\tFastball\t1,520\t53.6\t395\t338\t105\t59\t23\t2\t21\t73\t268\t.311\t.326\t.577\t.636\t.414\t.444\t95.7\t15\t24.9\t20.4\n",
    "# 2024\tBreaking\t866\t30.5\t216\t195\t55\t21\t9\t2\t23\t67\t129\t.282\t.275\t.703\t.665\t.435\t.425\t95.0\t20\t38.1\t23.0\n",
    "# 2024\tOffspeed\t452\t15.9\t110\t103\t37\t18\t6\t3\t10\t22\t82\t.359\t.348\t.767\t.731\t.480\t.468\t96.8\t14\t29.5\t17.6\n",
    "\n",
    "\n",
    "\n",
    "# input_folder = Path(\"data/raw/players-batting\")\n",
    "# output_csv = \"data/player_pitch_tracking_stats.csv\"\n",
    "# all_pitch_rows = []\n",
    "# for file_path in input_folder.glob(\"*.html\"):\n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         soup = BeautifulSoup(f, \"html.parser\")\n",
    "#     slug = file_path.stem\n",
    "#     table = soup.find(\"table\", id=\"detailedPitches\")\n",
    "#     if not table:\n",
    "#         continue\n",
    "#     headers = [th.get_text(strip=True).replace(\"\\n\", \" \") for th in table.find(\"thead\").find_all(\"th\")]\n",
    "#     for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "#         tds = tr.find_all(\"td\")\n",
    "#         row = [td.get_text(strip=True) for td in tds]\n",
    "#         if len(row) == len(headers):\n",
    "#             row_dict = dict(zip(headers, row))\n",
    "#             row_dict[\"Player Slug\"] = slug\n",
    "#             all_pitch_rows.append(row_dict)\n",
    "# df_pitch = pd.DataFrame(all_pitch_rows)\n",
    "# df_pitch.to_csv(output_csv, index=False)\n",
    "# print(f\"✅ Saved {len(df_pitch)} rows to {output_csv}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# input_folder = Path(\"data/raw/players-batting\")\n",
    "# output_csv = \"data/player_pitch_tracking_stats.csv\"\n",
    "# all_rows = []\n",
    "\n",
    "# # Count total files first\n",
    "# all_files = list(input_folder.glob(\"*.html\"))\n",
    "# total_files = len(all_files)\n",
    "# processed = 0\n",
    "# success = 0\n",
    "\n",
    "# print(f\"Processing {total_files} player files for pitch tracking stats...\")\n",
    "\n",
    "# for file_path in all_files:\n",
    "#     processed += 1\n",
    "#     if processed % 20 == 0 or processed == total_files:\n",
    "#         print(f\"Progress: {processed}/{total_files} files ({processed/total_files*100:.1f}%)\")\n",
    "    \n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         soup = BeautifulSoup(f, \"html.parser\")\n",
    "    \n",
    "#     slug = file_path.stem\n",
    "#     table = soup.find(\"table\", {\"id\": \"detailedPitches\"})\n",
    "#     if not table:\n",
    "#         continue\n",
    "\n",
    "#     # Extract column headers\n",
    "#     headers = [th.get_text(strip=True).replace(\"\\n\", \" \") for th in table.find(\"thead\").find_all(\"th\")]\n",
    "\n",
    "#     # Extract data rows\n",
    "#     for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "#         cells = tr.find_all(\"td\")\n",
    "#         row_data = [td.get_text(strip=True).replace(\",\", \"\") for td in cells]\n",
    "#         if len(row_data) == len(headers):\n",
    "#             row_dict = dict(zip(headers, row_data))\n",
    "#             row_dict[\"Player Slug\"] = slug\n",
    "#             all_rows.append(row_dict)\n",
    "#             success += 1\n",
    "\n",
    "# df = pd.DataFrame(all_rows)\n",
    "# df.to_csv(output_csv, index=False)\n",
    "# print(f\"✅ Saved {len(df)} rows to {output_csv}\")\n",
    "\n",
    "### Parse for Pitch Tracking Table ###\n",
    "\n",
    "\n",
    "\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# from bs4 import BeautifulSoup\n",
    "# from pathlib import Path\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# input_folder = Path(\"data/raw/players-batting\")\n",
    "# output_csv = Path(\"data/player_pitch_tracking_stats.csv\")\n",
    "# all_rows = []\n",
    "\n",
    "# all_files = list(input_folder.glob(\"*.html\"))\n",
    "# total_files = len(all_files)\n",
    "\n",
    "# print(f\"\\n🔍 Scanning {input_folder} for player HTML files...\\n\")\n",
    "\n",
    "# for idx, file_path in enumerate(tqdm(all_files, desc=\"Processing Players\")):\n",
    "#     slug = file_path.stem\n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         soup = BeautifulSoup(f.read(), \"html.parser\")\n",
    "\n",
    "#     # Try to find the <h2> heading for Pitch Tracking\n",
    "#     pitch_heading = soup.find(\"h2\", string=lambda s: s and \"Pitch Tracking\" in s)\n",
    "#     if not pitch_heading:\n",
    "#         print(f\"❌ {slug}: 'Pitch Tracking' section not found\")\n",
    "#         continue\n",
    "\n",
    "#     # Look for the table after the heading\n",
    "#     table = None\n",
    "#     next_tag = pitch_heading.find_next_sibling()\n",
    "#     while next_tag:\n",
    "#         if next_tag.name == \"div\" and next_tag.find(\"table\"):\n",
    "#             table = next_tag.find(\"table\")\n",
    "#             break\n",
    "#         next_tag = next_tag.find_next_sibling()\n",
    "\n",
    "#     if not table:\n",
    "#         print(f\"❌ {slug}: Pitch Tracking table not found\")\n",
    "#         continue\n",
    "\n",
    "#     # Extract headers\n",
    "#     header_cells = table.find(\"thead\").find_all(\"th\")\n",
    "#     headers = [cell.get_text(strip=True) for cell in header_cells]\n",
    "\n",
    "#     # Extract data rows\n",
    "#     for row in table.find(\"tbody\").find_all(\"tr\"):\n",
    "#         cells = row.find_all(\"td\")\n",
    "#         if not cells:\n",
    "#             continue\n",
    "#         values = [td.get_text(strip=True) for td in cells]\n",
    "#         if len(values) != len(headers):\n",
    "#             continue\n",
    "#         row_dict = dict(zip(headers, values))\n",
    "#         row_dict[\"Player Slug\"] = slug\n",
    "#         all_rows.append(row_dict)\n",
    "\n",
    "# # Save output to CSV\n",
    "# df = pd.DataFrame(all_rows)\n",
    "# df.to_csv(output_csv, index=False)\n",
    "# print(f\"\\n✅ Done. Extracted {len(df)} total rows into {output_csv}\")\n",
    "\n",
    "\n",
    "\n",
    "# ASYNCIO VERSION # \n",
    "\n",
    "\n",
    "# import asyncio\n",
    "# import aiohttp\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "# from tqdm import tqdm\n",
    "# import time\n",
    "# import nest_asyncio\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "# from functools import partial\n",
    "\n",
    "# # Apply nest_asyncio to allow async in Jupyter\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# # Setup paths and read player data\n",
    "# df_players = pd.read_csv(\"data/mlb_players.csv\")\n",
    "# output_csv = \"data/player_detailed_pitches.csv\"\n",
    "# all_rows = []\n",
    "\n",
    "# # Setup Chrome options optimized for M1/M2/M4\n",
    "# options = Options()\n",
    "# options.add_argument(\"--headless=new\")  # New headless mode\n",
    "# options.add_argument(\"--disable-gpu\")\n",
    "# options.add_argument(\"--no-sandbox\")\n",
    "# options.add_argument(\"--disable-dev-shm-usage\")\n",
    "# options.add_argument(\"--disable-features=TranslateUI\")\n",
    "# options.add_argument(\"--disable-extensions\")\n",
    "# options.add_argument(\"--disable-component-extensions-with-background-pages\")\n",
    "# options.add_argument(\"--disable-default-apps\")\n",
    "# options.add_argument(\"--enable-features=NetworkService,NetworkServiceInProcess\")\n",
    "\n",
    "# # Configure chunk size for parallel processing\n",
    "# CHUNK_SIZE = 5  # Process 5 players simultaneously\n",
    "\n",
    "# async def process_player(player_id, player_name, driver):\n",
    "#     try:\n",
    "#         url = f\"https://baseballsavant.mlb.com/savant-player/{player_id}?stats=statcast-r-hitting-mlb\"\n",
    "        \n",
    "#         driver.get(url)\n",
    "#         await asyncio.sleep(1)  # Reduced wait time\n",
    "        \n",
    "#         soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        \n",
    "#         # Find table by its specific ID\n",
    "#         table = soup.find(\"table\", id=\"detailedPitches\")\n",
    "#         if not table:\n",
    "#             print(f\"No pitch data found for {player_id}\")\n",
    "#             return []\n",
    "            \n",
    "#         # Extract headers\n",
    "#         headers = []\n",
    "#         for th in table.find(\"thead\").find_all(\"th\"):\n",
    "#             header = th.get_text(strip=True)\n",
    "#             header = header.replace(\"\\n\", \" \").strip()\n",
    "#             headers.append(header)\n",
    "        \n",
    "#         player_rows = []\n",
    "#         # Process each row\n",
    "#         for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "#             cells = tr.find_all(\"td\")\n",
    "#             if not cells:\n",
    "#                 continue\n",
    "                \n",
    "#             row_data = {\"Player ID\": player_id}\n",
    "            \n",
    "#             for idx, cell in enumerate(cells):\n",
    "#                 if idx < len(headers):\n",
    "#                     value = cell.get_text(strip=True).replace(\",\", \"\")\n",
    "#                     row_data[headers[idx]] = value\n",
    "                    \n",
    "#             if len(row_data) > 1:\n",
    "#                 player_rows.append(row_data)\n",
    "                \n",
    "#         return player_rows\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error processing {player_id}: {str(e)}\")\n",
    "#         return []\n",
    "\n",
    "# async def process_chunk(chunk_df, chunk_idx):\n",
    "#     driver = webdriver.Chrome(options=options)\n",
    "#     chunk_rows = []\n",
    "    \n",
    "#     try:\n",
    "#         for _, row in chunk_df.iterrows():\n",
    "#             player_rows = await process_player(row[\"Player ID\"], row[\"Player Name\"], driver)\n",
    "#             chunk_rows.extend(player_rows)\n",
    "            \n",
    "#         return chunk_rows\n",
    "#     finally:\n",
    "#         driver.quit()\n",
    "\n",
    "# async def main():\n",
    "#     chunks = [df_players[i:i + CHUNK_SIZE] for i in range(0, len(df_players), CHUNK_SIZE)]\n",
    "#     print(f\"\\n🔍 Processing {len(df_players)} players in {len(chunks)} chunks...\\n\")\n",
    "    \n",
    "#     all_rows = []\n",
    "    \n",
    "#     for chunk_idx, chunk_df in enumerate(tqdm(chunks, desc=\"Processing Chunks\")):\n",
    "#         chunk_rows = await process_chunk(chunk_df, chunk_idx)\n",
    "#         all_rows.extend(chunk_rows)\n",
    "        \n",
    "#         # Save progress after each chunk\n",
    "#         if chunk_rows:\n",
    "#             temp_df = pd.DataFrame(all_rows)\n",
    "#             first_cols = [\"Player ID\", \"Year\", \"Pitch Type\"]\n",
    "#             other_cols = [col for col in temp_df.columns if col not in first_cols]\n",
    "#             temp_df = temp_df[first_cols + sorted(other_cols)]\n",
    "#             temp_df.to_csv(output_csv, index=False)\n",
    "#             print(f\"\\n💾 Progress saved: {len(all_rows)} total rows\")\n",
    "    \n",
    "#     return all_rows\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         all_rows = asyncio.run(main())\n",
    "        \n",
    "#         if all_rows:\n",
    "#             df = pd.DataFrame(all_rows)\n",
    "#             first_cols = [\"Player ID\", \"Year\", \"Pitch Type\"]\n",
    "#             other_cols = [col for col in df.columns if col not in first_cols]\n",
    "#             df = df[first_cols + sorted(other_cols)]\n",
    "#             df.to_csv(output_csv, index=False)\n",
    "#             print(f\"\\n✅ Saved {len(df)} rows to {output_csv}\")\n",
    "#         else:\n",
    "#             print(\"\\n⚠️ No data was collected\")\n",
    "            \n",
    "#     except KeyboardInterrupt:\n",
    "#         print(\"\\n\\n⚠️ Process interrupted by user. Saving collected data...\")\n",
    "#         if all_rows:\n",
    "#             df = pd.DataFrame(all_rows)\n",
    "#             df.to_csv(output_csv, index=False)\n",
    "#             print(f\"Saved {len(df)} rows to {output_csv}\")\n",
    "\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import nest_asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "# Apply nest_asyncio to allow async in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Setup paths and read player data\n",
    "df_players = pd.read_csv(\"data/mlb_players.csv\")\n",
    "output_csv = \"data/player_detailed_pitches.csv\"\n",
    "all_rows = []\n",
    "players_processed = 0\n",
    "\n",
    "# Define the exact columns we want in order\n",
    "COLUMNS = [\n",
    "    \"Player ID\", \"Year\", \"Pitch Type\", \"#\", \"%\", \"PA\", \"AB\", \"H\", \"1B\", \"2B\", \n",
    "    \"3B\", \"HR\", \"SO\", \"BBE\", \"BA\", \"XBA\", \"SLG\", \"XSLG\", \"WOBA\", \"XWOBA\", \n",
    "    \"EV\", \"LA\", \"Whiff%\", \"PutAway%\"\n",
    "]\n",
    "\n",
    "# Setup Chrome options optimized for M1/M2/M4\n",
    "options = Options()\n",
    "options.add_argument(\"--headless=new\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--disable-features=TranslateUI\")\n",
    "options.add_argument(\"--disable-extensions\")\n",
    "options.add_argument(\"--disable-component-extensions-with-background-pages\")\n",
    "options.add_argument(\"--disable-default-apps\")\n",
    "options.add_argument(\"--enable-features=NetworkService,NetworkServiceInProcess\")\n",
    "options.add_argument(\"--disable-javascript-harmony-shipping\")\n",
    "options.add_argument(\"--disable-site-isolation-trials\")\n",
    "options.add_argument(\"--ignore-certificate-errors\")\n",
    "options.add_argument(\"--disable-web-security\")\n",
    "options.add_argument(\"--disable-features=IsolateOrigins,site-per-process\")\n",
    "options.add_argument(\"--aggressive-cache-discard\")\n",
    "options.add_argument(\"--disable-cache\")\n",
    "options.add_argument(\"--disable-application-cache\")\n",
    "options.add_argument(\"--disable-offline-load-stale-cache\")\n",
    "options.add_argument(\"--disk-cache-size=0\")\n",
    "options.page_load_strategy = 'none'  # Changed from 'eager' to 'none' for fastest possible loading\n",
    "\n",
    "# Configure parallel processing\n",
    "NUM_WORKERS = 20  # Increased workers\n",
    "CHUNK_SIZE = max(1, len(df_players) // (NUM_WORKERS * 2))\n",
    "SAVE_FREQUENCY = 25  # Save every 25 players\n",
    "\n",
    "# Minimum viable timeouts (in seconds)\n",
    "PAGE_LOAD_TIMEOUT = 2\n",
    "SCRIPT_TIMEOUT = 2\n",
    "ELEMENT_TIMEOUT = 2\n",
    "\n",
    "# XPath constants\n",
    "TABLE_XPATH = \"/html/body/div[2]/section/div/section/div[5]/section/div[1]/div[6]/div[2]/div/table\"\n",
    "THEAD_XPATH = f\"{TABLE_XPATH}/thead/tr\"\n",
    "TBODY_XPATH = f\"{TABLE_XPATH}/tbody\"\n",
    "\n",
    "class PlayerDataExtractor:\n",
    "    def __init__(self):\n",
    "        self.driver = None\n",
    "        self.wait = None\n",
    "        \n",
    "    async def setup(self):\n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "        self.driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)\n",
    "        self.driver.set_script_timeout(SCRIPT_TIMEOUT)\n",
    "        self.wait = WebDriverWait(self.driver, ELEMENT_TIMEOUT)\n",
    "        \n",
    "    async def cleanup(self):\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            \n",
    "    async def get_table_data(self, player_id: str, player_name: str) -> List[Dict]:\n",
    "        try:\n",
    "            url = f\"https://baseballsavant.mlb.com/savant-player/{player_id}?stats=statcast-r-hitting-mlb\"\n",
    "            try:\n",
    "                self.driver.get(url)\n",
    "            except TimeoutException:\n",
    "                # If timeout occurs, the page might still be usable\n",
    "                pass\n",
    "                \n",
    "            try:\n",
    "                table = self.wait.until(EC.presence_of_element_located((By.XPATH, TABLE_XPATH)))\n",
    "            except TimeoutException:\n",
    "                # One more quick attempt with a shorter timeout\n",
    "                self.wait = WebDriverWait(self.driver, 1)\n",
    "                table = self.wait.until(EC.presence_of_element_located((By.XPATH, TABLE_XPATH)))\n",
    "            \n",
    "            # Extract headers immediately without waiting\n",
    "            header_map = {}\n",
    "            header_elements = self.driver.find_elements(By.XPATH, f\"{THEAD_XPATH}/th\")\n",
    "            for idx, elem in enumerate(header_elements):\n",
    "                try:\n",
    "                    header = elem.find_element(By.TAG_NAME, \"div\").text.strip()\n",
    "                except:\n",
    "                    header = elem.text.strip()\n",
    "                header = header.replace(\"\\n\", \" \").strip()\n",
    "                if header in COLUMNS:\n",
    "                    header_map[idx] = header\n",
    "            \n",
    "            # Extract rows without explicit wait\n",
    "            tbody = self.driver.find_element(By.XPATH, TBODY_XPATH)\n",
    "            rows = tbody.find_elements(By.TAG_NAME, \"tr\")\n",
    "            \n",
    "            player_rows = []\n",
    "            for row in rows:\n",
    "                cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                if not cells:\n",
    "                    continue\n",
    "                \n",
    "                row_data = {col: \"\" for col in COLUMNS}\n",
    "                row_data[\"Player ID\"] = player_id\n",
    "                has_data = False\n",
    "                \n",
    "                for idx, cell in enumerate(cells):\n",
    "                    if idx in header_map:\n",
    "                        value = cell.text.strip().replace(\",\", \"\")\n",
    "                        if value:\n",
    "                            row_data[header_map[idx]] = value\n",
    "                            has_data = True\n",
    "                \n",
    "                if has_data:\n",
    "                    player_rows.append(row_data)\n",
    "            \n",
    "            return player_rows\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {player_name}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "async def process_chunk(chunk_df, worker_id, progress_bar):\n",
    "    global players_processed, all_rows\n",
    "    extractor = PlayerDataExtractor()\n",
    "    chunk_rows = []\n",
    "    \n",
    "    try:\n",
    "        await extractor.setup()\n",
    "        \n",
    "        for _, row in chunk_df.iterrows():\n",
    "            try:\n",
    "                player_rows = await extractor.get_table_data(row[\"Player ID\"], row[\"Player Name\"])\n",
    "                if player_rows:\n",
    "                    chunk_rows.extend(player_rows)\n",
    "                players_processed += 1\n",
    "                progress_bar.update(1)\n",
    "                \n",
    "                # Save progress periodically\n",
    "                if players_processed % SAVE_FREQUENCY == 0:\n",
    "                    await save_progress(chunk_rows)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in worker {worker_id}: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "        return chunk_rows\n",
    "    finally:\n",
    "        await extractor.cleanup()\n",
    "\n",
    "async def save_progress(new_rows):\n",
    "    if not new_rows:\n",
    "        return\n",
    "        \n",
    "    temp_df = pd.DataFrame(all_rows + new_rows)\n",
    "    if not temp_df.empty:\n",
    "        temp_df = temp_df[COLUMNS]\n",
    "        temp_df.to_csv(output_csv, index=False)\n",
    "\n",
    "async def main():\n",
    "    chunks = [df_players[i:i + CHUNK_SIZE] for i in range(0, len(df_players), CHUNK_SIZE)]\n",
    "    print(f\"\\n🔍 Processing {len(df_players)} players using {NUM_WORKERS} workers ({len(chunks)} chunks)...\\n\")\n",
    "    \n",
    "    # Create progress bar for total players\n",
    "    progress_bar = tqdm(total=len(df_players), desc=\"Processing Players\")\n",
    "    \n",
    "    # Create semaphore to limit concurrent Chrome instances\n",
    "    sem = asyncio.Semaphore(NUM_WORKERS)\n",
    "    \n",
    "    async def process_chunk_with_semaphore(chunk_df, worker_id):\n",
    "        async with sem:\n",
    "            return await process_chunk(chunk_df, worker_id, progress_bar)\n",
    "    \n",
    "    # Process chunks concurrently\n",
    "    tasks = [\n",
    "        asyncio.create_task(process_chunk_with_semaphore(chunk_df, i))\n",
    "        for i, chunk_df in enumerate(chunks)\n",
    "    ]\n",
    "    \n",
    "    # Wait for all tasks to complete\n",
    "    chunk_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    # Combine results\n",
    "    for result in chunk_results:\n",
    "        if isinstance(result, list):\n",
    "            all_rows.extend(result)\n",
    "    \n",
    "    progress_bar.close()\n",
    "    return all_rows\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        all_rows = asyncio.run(main())\n",
    "        \n",
    "        if all_rows:\n",
    "            df = pd.DataFrame(all_rows)\n",
    "            df = df[COLUMNS]\n",
    "            df.to_csv(output_csv, index=False)\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"\\n✅ Completed in {elapsed_time:.2f} seconds\")\n",
    "            print(f\"✅ Processed {players_processed} players\")\n",
    "            print(f\"✅ Saved {len(df)} rows to {output_csv}\")\n",
    "        else:\n",
    "            print(\"\\n⚠️ No data was collected\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n⚠️ Process interrupted by user. Saving collected data...\")\n",
    "        if all_rows:\n",
    "            df = pd.DataFrame(all_rows)\n",
    "            df = df[COLUMNS]\n",
    "            df.to_csv(output_csv, index=False)\n",
    "            print(f\"Saved {len(df)} rows to {output_csv}\")\n",
    "            \n",
    "            \n",
    "!open data/player_pitch_tracking_stats.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca47267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Batted Ball Profile Table\n",
    "# # Quality Of Contact Table\n",
    "\n",
    "\n",
    "# # Input and output paths\n",
    "# input_folder = Path(\"data/raw/players-batting\")  # folder with .html player files\n",
    "# output_profile = \"data/player_batted_ball_profile.csv\"\n",
    "# output_quality = \"data/player_quality_of_contact.csv\"\n",
    "\n",
    "# all_profile_rows = []\n",
    "# all_quality_rows = []\n",
    "\n",
    "# # Helper function to parse tables\n",
    "# def parse_table(table, slug):\n",
    "#     headers = [th.get_text(strip=True) for th in table.find(\"thead\").find_all(\"th\")]\n",
    "#     rows = []\n",
    "#     for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "#         cells = tr.find_all([\"td\", \"th\"])\n",
    "#         values = [td.get_text(strip=True) for td in cells]\n",
    "#         if len(values) == len(headers):\n",
    "#             row_dict = dict(zip(headers, values))\n",
    "#             row_dict[\"Player Slug\"] = slug\n",
    "#             rows.append(row_dict)\n",
    "#     return rows\n",
    "\n",
    "# # Run test on uploaded files\n",
    "# html_files = [\n",
    "#     (\"aaron-judge-592450.html\", \"aaron-judge-592450\"),\n",
    "#     (\"brandon-lowe-664040.html\", \"brandon-lowe-664040\"),\n",
    "# ]\n",
    "\n",
    "# for filename, slug in html_files:\n",
    "#     with open(f\"/mnt/data/{filename}\", \"r\", encoding=\"utf-8\") as f:\n",
    "#         soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "#     tables = soup.find_all(\"table\")\n",
    "#     if len(tables) >= 2:\n",
    "#         all_profile_rows.extend(parse_table(tables[0], slug))\n",
    "#         all_quality_rows.extend(parse_table(tables[1], slug))\n",
    "\n",
    "# # Convert to DataFrames\n",
    "# df_profile = pd.DataFrame(all_profile_rows)\n",
    "# df_quality = pd.DataFrame(all_quality_rows)\n",
    "\n",
    "# # Show output here for verification\n",
    "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Batted Ball Profile Table\", dataframe=df_profile)\n",
    "# tools.display_dataframe_to_user(name=\"Quality of Contact Table\", dataframe=df_quality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c585e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Batted Ball v2\n",
    "\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# input_folder = Path(\"Scrapers/data/raw/players-batting\")\n",
    "# output_file = Path(\"data/player_pitch_tracking_stats.csv\")\n",
    "\n",
    "# all_rows = []\n",
    "# files_processed = 0\n",
    "# files_with_data = 0\n",
    "\n",
    "# # Keep track of the first found table's columns to use as reference\n",
    "# reference_columns = []\n",
    "\n",
    "# print(f\"🔍 Scanning {input_folder} for player HTML files...\\n\")\n",
    "\n",
    "# all_files = list(input_folder.glob(\"*.html\"))\n",
    "# print(f\"Found {len(all_files)} HTML files\")\n",
    "\n",
    "# for file_path in tqdm(all_files, desc=\"Processing players\"):\n",
    "#     files_processed += 1\n",
    "    \n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         soup = BeautifulSoup(f.read(), \"html.parser\")\n",
    "\n",
    "#     player_id = file_path.stem\n",
    "#     print(f\"\\nProcessing {player_id}...\")\n",
    "\n",
    "#     # First try the standard detailedPitches ID\n",
    "#     table = soup.find(\"table\", {\"id\": \"detailedPitches\"})\n",
    "\n",
    "#     # If not found, look for h2 elements containing \"Pitch Tracking\" text and find the next table\n",
    "#     if not table:\n",
    "#         pitch_tracking_headers = soup.find_all(lambda tag: tag.name == \"h2\" and \"Pitch Tracking\" in tag.get_text())\n",
    "#         for header in pitch_tracking_headers:\n",
    "#             table = header.find_next(\"table\")\n",
    "#             if table:\n",
    "#                 break\n",
    "\n",
    "#     # If still not found, search for tables with specific column headers\n",
    "#     if not table:\n",
    "#         candidate_tables = soup.find_all(\"table\")\n",
    "#         for t in candidate_tables:\n",
    "#             headers = [th.get_text(strip=True) for th in t.find_all(\"th\")]\n",
    "#             if \"Pitch Type\" in headers and \"Year\" in headers:\n",
    "#                 table = t\n",
    "#                 break\n",
    "\n",
    "#     if not table:\n",
    "#         print(f\"❌ No pitch tracking table found for {player_id}\")\n",
    "#         continue\n",
    "\n",
    "#     # Extract headers in their original order\n",
    "#     header_cells = table.find_all(\"th\")\n",
    "#     headers = [cell.get_text(strip=True) for cell in header_cells]\n",
    "    \n",
    "#     # Store the first table's columns as reference if not already set\n",
    "#     if not reference_columns and len(headers) > 0:\n",
    "#         reference_columns = headers.copy()\n",
    "#         print(f\"📊 Using column order from first found table: {reference_columns}\")\n",
    "\n",
    "#     # Extract rows\n",
    "#     rows_found = 0\n",
    "#     for tr in table.find(\"tbody\").find_all(\"tr\") if table.find(\"tbody\") else table.find_all(\"tr\"):\n",
    "#         cells = tr.find_all(\"td\")\n",
    "#         if not cells or len(cells) <= 1:\n",
    "#             continue\n",
    "            \n",
    "#         # Skip \"Show More Seasons\" rows\n",
    "#         if len(cells) == 1 and \"Show More\" in cells[0].get_text():\n",
    "#             continue\n",
    "            \n",
    "#         # Skip note rows about years in reverse order\n",
    "#         if len(cells) == 1 and \"Note:\" in cells[0].get_text():\n",
    "#             continue\n",
    "\n",
    "#         row_data = {\"Player ID\": player_id}\n",
    "#         for i, cell in enumerate(cells):\n",
    "#             if i < len(headers):\n",
    "#                 span = cell.find(\"span\")\n",
    "#                 text = span.get_text(strip=True) if span else cell.get_text(strip=True)\n",
    "                \n",
    "#                 # Add to row data using original header names\n",
    "#                 row_data[headers[i]] = text\n",
    "\n",
    "#         # Only add rows that have actual data\n",
    "#         if len(row_data) > 1:  # More than just Player ID\n",
    "#             all_rows.append(row_data)\n",
    "#             rows_found += 1\n",
    "\n",
    "#     if rows_found > 0:\n",
    "#         files_with_data += 1\n",
    "#         print(f\"✓ Extracted {rows_found} rows for {player_id}\")\n",
    "#     else:\n",
    "#         print(f\"⚠️ No rows found for {player_id}\")\n",
    "    \n",
    "#     # Save every 20 files\n",
    "#     if files_processed % 20 == 0 and all_rows:\n",
    "#         # Create intermediate save\n",
    "#         temp_df = pd.DataFrame(all_rows)\n",
    "        \n",
    "#         # Clean up numeric values\n",
    "#         for col in temp_df.columns:\n",
    "#             if temp_df[col].dtype == object:  # Only process string columns\n",
    "#                 temp_df[col] = temp_df[col].astype(str).str.replace(\",\", \"\")\n",
    "        \n",
    "#         # Filter out rows where Year is \"Player\" or \"MLB\"\n",
    "#         if \"Year\" in temp_df.columns:\n",
    "#             temp_df = temp_df[~temp_df[\"Year\"].isin([\"Player\", \"MLB\"])]\n",
    "        \n",
    "#         # Force column order to match original table\n",
    "#         cols_to_use = [\"Player ID\"]\n",
    "#         for col in reference_columns:\n",
    "#             if col in temp_df.columns:\n",
    "#                 cols_to_use.append(col)\n",
    "        \n",
    "#         # Add any columns that might be in the data but not in our reference\n",
    "#         for col in temp_df.columns:\n",
    "#             if col not in cols_to_use:\n",
    "#                 cols_to_use.append(col)\n",
    "                \n",
    "#         temp_df = temp_df[cols_to_use]\n",
    "            \n",
    "#         # Save intermediate result\n",
    "#         temp_df.to_csv(output_file, index=False)\n",
    "#         print(f\"💾 Intermediate save: {len(all_rows)} rows saved after processing {files_processed} files\")\n",
    "\n",
    "# print(f\"\\n🏁 Processing complete!\")\n",
    "# print(f\"Files processed: {files_processed}\")\n",
    "# print(f\"Files with pitch tracking data: {files_with_data}\")\n",
    "# print(f\"Total rows collected: {len(all_rows)}\")\n",
    "\n",
    "# if all_rows:\n",
    "#     # Create final DataFrame\n",
    "#     df = pd.DataFrame(all_rows)\n",
    "    \n",
    "#     # Clean up numeric values\n",
    "#     for col in df.columns:\n",
    "#         if df[col].dtype == object:  # Only process string columns\n",
    "#             df[col] = df[col].astype(str).str.replace(\",\", \"\")\n",
    "    \n",
    "#     # Filter out rows where Year is \"Player\" or \"MLB\"\n",
    "#     if \"Year\" in df.columns:\n",
    "#         df = df[~df[\"Year\"].isin([\"Player\", \"MLB\"])]\n",
    "    \n",
    "#     # Force column order to match original table\n",
    "#     cols_to_use = [\"Player ID\"]\n",
    "#     for col in reference_columns:\n",
    "#         if col in df.columns:\n",
    "#             cols_to_use.append(col)\n",
    "    \n",
    "#     # Add any columns that might be in the data but not in our reference\n",
    "#     for col in df.columns:\n",
    "#         if col not in cols_to_use:\n",
    "#             cols_to_use.append(col)\n",
    "    \n",
    "#     # Reorder columns according to our list\n",
    "#     df = df[cols_to_use]\n",
    "    \n",
    "#     # Make sure output directory exists\n",
    "#     output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "#     df.to_csv(output_file, index=False)\n",
    "#     print(f\"💾 Final data saved to {output_file}\")\n",
    "#     print(f\"Final column order: {list(df.columns)}\")\n",
    "    \n",
    "#     if len(df) > 0:\n",
    "#         print(f\"Sample of extracted data:\")\n",
    "#         print(df.head(2))\n",
    "# else:\n",
    "#     print(\"⚠️ No pitch tracking data found to save\")\n",
    "\n",
    "# # import ace_tools as tools; tools.display_dataframe_to_user(name=\"Pitch Tracking Stats\", dataframe=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01a37e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bdef71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ebec88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc836f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bba43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# from bs4 import BeautifulSoup\n",
    "# from pathlib import Path\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# input_folder = Path(\"data/raw/players-batting\")\n",
    "# output_file = Path(\"data/player_pitch_tracking_stats.csv\")\n",
    "\n",
    "# all_rows = []\n",
    "# all_columns = set()\n",
    "\n",
    "# # Loop through all player HTML files\n",
    "# for file_path in tqdm(list(input_folder.glob(\"*.html\")), desc=\"Processing players\"):\n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         soup = BeautifulSoup(f.read(), \"html.parser\")\n",
    "\n",
    "#     player_id = file_path.stem\n",
    "\n",
    "#     # Locate the Pitch Tracking table\n",
    "#     table = soup.find(\"table\", {\"id\": \"detailedPitches\"})\n",
    "#     if not table:\n",
    "#         continue\n",
    "\n",
    "#     # Extract headers\n",
    "#     header_cells = table.find(\"thead\").find_all(\"th\")\n",
    "#     headers = [cell.get_text(strip=True) for cell in header_cells]\n",
    "#     all_columns.update(headers)\n",
    "\n",
    "#     # Extract rows\n",
    "#     for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "#         cells = tr.find_all(\"td\")\n",
    "#         if not cells:\n",
    "#             continue\n",
    "\n",
    "#         row_data = {}\n",
    "#         for i, cell in enumerate(cells):\n",
    "#             # Grab text inside <span> or directly from cell\n",
    "#             span = cell.find(\"span\")\n",
    "#             text = span.get_text(strip=True) if span else cell.get_text(strip=True)\n",
    "#             col_name = headers[i] if i < len(headers) else f\"Extra_{i}\"\n",
    "#             row_data[col_name] = text\n",
    "#             all_columns.add(col_name)\n",
    "\n",
    "#         row_data[\"Player ID\"] = player_id\n",
    "#         all_rows.append(row_data)\n",
    "\n",
    "# # Ensure consistent columns\n",
    "# final_columns = [\"Player ID\"] + sorted(all_columns - {\"Player ID\"})\n",
    "# df = pd.DataFrame(all_rows, columns=final_columns)\n",
    "\n",
    "# # Clean up numeric values (remove commas)\n",
    "# for col in df.columns:\n",
    "#     df[col] = df[col].str.replace(\",\", \"\", regex=False)\n",
    "\n",
    "# df.to_csv(output_file, index=False)\n",
    "# print(f\"✅ Done. Saved pitch tracking stats to: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed14362f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f78e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Pitching Stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c84e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda0705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05501702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
